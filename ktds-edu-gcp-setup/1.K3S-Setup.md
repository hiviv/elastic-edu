#  < K3S Setup >

GCP에 Instance Group 을 생성하고 k3s 를 Setup후 Load Balace, k9s 설정한다.

다수의 사용자들의 원할한 접근을 위해 GKE 대신 수작업으로 Kubernetes 를 설정한다.





# 1. VM(Instance Group) 설정



## 1) local 연결을 위한 SSH Key 생성

### (1) 관리자용 SSH key 생성

#### ssh keygen

```sh
$ mkdir -p ~/song/gcp-setup/sshkey/manager
$ cd ~/song/gcp-setup/sshkey/manager

# key 는 아래 경로로 만들자.
# /home/song/song/gcp-setup/sshkey/manager/ktdseduuser

$ ssh-keygen -t rsa -b 4096 -C ktdseduuser

Generating public/private rsa key pair.
Enter file in which to save the key (/root/.ssh/id_rsa): /home/song/song/gcp-setup/sshkey/manager/ktdseduuser
Enter passphrase (empty for no passphrase):
Enter same passphrase again:
Your identification has been saved in /home/song/song/gcp-setup/sshkey/manager/ktdseduuser
Your public key has been saved in /home/song/song/gcp-setup/sshkey/manager/ktdseduuser.pub
The key fingerprint is:
SHA256:RUWm8TG2Wb5sAnv8XzSa3Ig2GJ/2jXO7BygG2tPwYk0 ktdseduuser
The key's randomart image is:
+---[RSA 4096]----+
|          ooB .  |
|         . * B   |
|          + + .  |
|        o.E+ . . |
|       oSO. +.+..|
|      . = X.=+*..|
|       . = O =.o.|
|          o o.ooo|
|             oo+=|
+----[SHA256]-----+


## 비밀번호 : [emtpy]


$ ls -ltr
-rw-r--r-- 1 root root  737 May 13 15:01 ktdseduuser.pub
-rw------- 1 root root 3369 May 13 15:01 ktdseduuser

```



#### ssh 확인

```sh
$ cd ~/song/gcp-setup/sshkey/manager

$ ll
-rw------- 1 root root 3369 May 13 15:01 ktdseduuser
-rw-r--r-- 1 root root  737 May 13 15:01 ktdseduuser.pub


$ cat ktdseduuser
-----BEGIN OPENSSH PRIVATE KEY-----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-----END OPENSSH PRIVATE KEY-----


$ cat ktdseduuser.pub
ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQDVhqICvHjqGjvJ5lyedivtpjomZZU20qtO8+KGTWxDtyyxX5ho14fxNSA0T9zukpbvZ0rH0K38nAcRvaCBtZxbWGk4mxshBxBO1xi4gr/Igi81+7s92ByuJuiZVXformhx/5zfma16aIKOdPr0ffUIpjr6ocZQce9RwoR5pGYn3/QioCJSsDNbDE3leWs9ppC28u2PwmMq9ElnjbtHlL0yxAM5uLPVheWaF5e5x4266IptpJNELphTxeFzC2JIy2Sc5V0O/H728Str+ialszP+Ghv7WuwEc/E7eMT7dGflVy+j/5kcLgxREoxxns8JEDDTZ1QORmjOC9fpOuUTZwdFI6ppNscdIbQZ3x9vWEbPrtaJNAOl3gkYyyLiEnzTt7iImC+5NJ0t3qp2TDcrxc56Ig/drLHP1+Wnir9Ga2z3JAvBoMm88tBnBM3XqsO0r8bee59S85h68XYo2NwURPmYeggS/38jeoYKIwi3GAzJx12c450hSIxL21H6kSCLrZFn1YfPXwfVRtr7t4uo4Dpom9Q2AIR71onyu8REGl1umMPa6vO1MCGJf5yZKUxMiNzjqMWFgD/axOGkE1CtW7/5lmfzJoEkUfBHW4hZ3iPLC8q7ij7Rfvsz4eCbEOp8sK4olEwIohLNL0mqyJyzR1h06Zu3gMS/IOcCjmtw+r3qQQ== ktdseduuser



```



### (2) 교육생용 SSH key 생성

#### ssh keygen

```sh
$ mkdir -p ~/song/gcp-setup/sshkey/student
$ cd ~/song/gcp-setup/sshkey/student

# key 는 아래 경로로 만들자.
# /home/song/song/gcp-setup/sshkey/student/ktdseduuser

$ ssh-keygen -t rsa -b 4096 -C ktdseduuser

Generating public/private rsa key pair.
Enter file in which to save the key (/root/.ssh/id_rsa): /home/song/song/gcp-setup/sshkey/student/ktdseduuser
Enter passphrase (empty for no passphrase):
Enter same passphrase again:
Your identification has been saved in /home/song/song/gcp-setup/sshkey/student/ktdseduuser
Your public key has been saved in /home/song/song/gcp-setup/sshkey/student/ktdseduuser.pub
The key fingerprint is:
SHA256:GxcSwJTD/2EI8DHRPOfw4ehVVNsUmRp9rxfYEJ1Nw2Y ktdseduuser
The key's randomart image is:
+---[RSA 4096]----+
|    .=**.  ..o*+O|
|     .*o=.o .o E+|
|      .+.X.o  X +|
|        +.B. o o.|
|       .S+..   ..|
|        .+.   . .|
|        .      . |
|                 |
|                 |
+----[SHA256]-----+



## 비밀번호 : [emtpy]


$ ls -ltr
-rw------- 1 song song 3369 Aug 27 15:31 ktdseduuser
-rw-r--r-- 1 song song  737 Aug 27 15:31 ktdseduuser.pub


```



#### ssh 확인

```sh
$ cd ~/song/gcp-setup/sshkey/student

$ ll
-rw------- 1 song song 3369 Aug 27 15:31 ktdseduuser
-rw-r--r-- 1 song song  737 Aug 27 15:31 ktdseduuser.pub



$ cat ktdseduuser
-----BEGIN OPENSSH PRIVATE KEY-----
b3BlbnNzaC1rZXktdjEAAAAABG5vbmUAAAAEbm9uZQAAAAAAAAABAAACFwAAAAdzc2gtcn
NhAAAAAwEAAQAAAgEA8OeTiDWpy3+48cOBWp51hjahb4V/cs2rgpYiL1ns86/RqND3qWkC
knkXAP3CkPbSuxPwLkk2aKBwu4lWwwgHG7b+9osnGJKY3ZN61NvrvH1qp2VvIodSWEiSLk
RJsT6kGSzItUG4ggYZipI5Kt2AIGjRSPpDMPS6RCH/XSIDZ36jhs238qfigjq8pk6GZCEC
lQgHbcXjfXmuSx6SHdgmzvYCTDq0mkAyHM//STuo0YzMBs4eg+g0HZ2Z2tNviPwh3B2t2o
IYzxIRu4zJKyAn6MESuf/UuuUulx8te8NClO3wRhw+UzjVfjFHPS0otguBNV1u/55OPha4
Ai0rAswNFT8aCdfZFC+QmrzM0+3Dmt7HAJ2kTHjXRrGrEUlHC29VrHYohLDUKiNLBuC1xD
x2dsuYTvNVOV9xjeNhlQwJqEXVUIo1mdOzzpKrYsMpG5Mp4p1L7U4gHWxT9rVuL/CZXM9w
BI+Q4Jw5N/En0muz6rW1NI4vUfd+4nRMLSBoLd1r163AM6BbSu+O/B/yJviAeY/G0JhC0A
ejLOcTPZr/w0vwA3cVQ+jYZN0xSOnH9cyCD/BMJyVh6JoSTlMyu2ADvDb4K4kv14y3BVja
9Kw2Q6rAZeTtPXcShYyw3oq8up5dwDnKAeooyGPDXywxIPLjJFLrhJDLrADmsLutNZpZK1
MAAAdADLPZOgyz2ToAAAAHc3NoLXJzYQAAAgEA8OeTiDWpy3+48cOBWp51hjahb4V/cs2r
gpYiL1ns86/RqND3qWkCknkXAP3CkPbSuxPwLkk2aKBwu4lWwwgHG7b+9osnGJKY3ZN61N
vrvH1qp2VvIodSWEiSLkRJsT6kGSzItUG4ggYZipI5Kt2AIGjRSPpDMPS6RCH/XSIDZ36j
hs238qfigjq8pk6GZCEClQgHbcXjfXmuSx6SHdgmzvYCTDq0mkAyHM//STuo0YzMBs4eg+
g0HZ2Z2tNviPwh3B2t2oIYzxIRu4zJKyAn6MESuf/UuuUulx8te8NClO3wRhw+UzjVfjFH
PS0otguBNV1u/55OPha4Ai0rAswNFT8aCdfZFC+QmrzM0+3Dmt7HAJ2kTHjXRrGrEUlHC2
9VrHYohLDUKiNLBuC1xDx2dsuYTvNVOV9xjeNhlQwJqEXVUIo1mdOzzpKrYsMpG5Mp4p1L
7U4gHWxT9rVuL/CZXM9wBI+Q4Jw5N/En0muz6rW1NI4vUfd+4nRMLSBoLd1r163AM6BbSu
+O/B/yJviAeY/G0JhC0AejLOcTPZr/w0vwA3cVQ+jYZN0xSOnH9cyCD/BMJyVh6JoSTlMy
u2ADvDb4K4kv14y3BVja9Kw2Q6rAZeTtPXcShYyw3oq8up5dwDnKAeooyGPDXywxIPLjJF
LrhJDLrADmsLutNZpZK1MAAAADAQABAAACAC8XZayLqOxhxDpmWEPpk9mvljEhrH85mxxE
0tvkvUAJj3drme4g2+LcDtYfxMfnp9vwEtRJdXxQWJkgSLiJOjJ9vCtfmj3gckdmC+3OK6
/F6HJejrH2N++QCBtJOAqQzy0opvdfqHyqSpXe7ZPmhqRPORIF8dqBDGRDLRL8kiPKCpdQ
YXPqw4lnh/cOhJDQLxwI3MLlmwmtHpgKms+AsF1mk55pH2LDx8gAFMwxAFguKLfv1OE+I+
niI/A1wqjJeKj5mEGNiuH3i0XZSH3IpGSIUWvQibobENdzvcVirwJD3OMnpDtKcdlgMqfP
yxSC/Tf8UrVfMbZLZJ7wS2cZkjjC569h3vv7j03Cg1W93sfkAB+vPOC+cnDS88806iK6Sr
9T60LXd3NX8cqDNf9zsRVSaLkv9TXXfFYGxQtkvIpeTc5gsCsqbji3B6xKDiKs+Yuw7cc1
0xfWcllUywiJ41vsOBKDu2IZGGEc+Edj1vyGDmjic/du7e/ly8Cm8RMUwQBjYAgRlztffD
cMi033SB0OHF2B1meGhZ3TED/5JZSqcZ2mG+2/DVVLIj9xbLZflqhWN0gPEsHREMncYvQn
IGI2K1VHiM/xyk6PMwtvr+82x8j606F3HanGlJ0cJ/vNJfuSN1R7wEF6KZjQGxFpbcvUSU
P9C8UUl3FXG594DtoJAAABADKUMgUroqid2JUJSZjRwFg6nP7mx57nC33apnEh/z7jsshI
oCjHPe3u9M03MlRJJWMQPKzUVktNjGCUJ90TF+oYmjad5loFPNuzKcah/Ak/tFbGRSLeRd
fyD6GmdgWxvj+jLP0N1VXDKYJWfyy2BD6MpjESNTHVeZgGlFXK8DuOUdgxY8h1hjXBvYrq
lxX3+TgLFRzUJSwSCxXQMOW9DHkY7CqyJxZqTFGMiUgyi0KGSjY+3maDyaP2HhbX/7rtIK
Pvvke+Bt+33DDfFKWl23K8PlbKbCkk7ettrbwxL1+jkEPcmxtXQ3NQx1eSNfTngqP4Qlhz
afnmlmbpWtsnpHAAAAEBAP1+KyC1tiNX1/VBqcit6ALZl3VzYrB4X2LDQHu3ymEFyg3FUD
kTiid6npl0Uzg9vf2xgthqDDDl5WDwzf2jGwNay4nkgPw0FJvhyO/kM3JIf4So0bSyLTzN
IZ5QAIwzW1d/jvY1UyE8RPK88SQyxe1AMI0jVBaWt8Dikk/t/nhDDxy921Sggu1dPB9Pmn
pZ6FHcr7rVSskn89DZ9H3SxZDqifsxEjMwdfxmJCuCMCl0v4lWxoYlkDkfZ0RauoZK05tJ
Xs3FIEJB0ikMdIFqNTXtn04pHJ5KNHrhjIkbHKMWI5ZJNGyVzGmrAYeg//bXmmQXRXa+zB
qtfdLPo1GLxBcAAAEBAPNJiPU8ydge6qs53pDSXH9oF3XAz1bDwF+yzErYVqYXlJTRS42q
8oZaMyTa4FjWkOOFphqrwUfXGmjXU3/7+zVTjzPcj2NPn/YZr4g0hrW/gL3EDoS9vITt99
uKtIoRcTNwMeaxdlSr5Qnwk/GCxmc98bPbLQpDkltqRnP2VoZjnC1TkPRmDgi3i1pAORza
SUJcwewoLD31fbG6iKbENHaI3BrNou7D7BI4YMxWXJ8DzcHJv9pjeUtBYGkCjlPJY0BuKp
L4Q8cvg0msHhOraBCmQkAbv0aV5vS1Albla3E+IClj2oSIjHbi7qTntAc7NnFqO2svdlze
+k6opTbpTCUAAAALa3Rkc2VkdXVzZXI=
-----END OPENSSH PRIVATE KEY-----




$ cat ktdseduuser.pub
ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQDw55OINanLf7jxw4FannWGNqFvhX9yzauCliIvWezzr9Go0PepaQKSeRcA/cKQ9tK7E/AuSTZooHC7iVbDCAcbtv72iycYkpjdk3rU2+u8fWqnZW8ih1JYSJIuREmxPqQZLMi1QbiCBhmKkjkq3YAgaNFI+kMw9LpEIf9dIgNnfqOGzbfyp+KCOrymToZkIQKVCAdtxeN9ea5LHpId2CbO9gJMOrSaQDIcz/9JO6jRjMwGzh6D6DQdnZna02+I/CHcHa3aghjPEhG7jMkrICfowRK5/9S65S6XHy17w0KU7fBGHD5TONV+MUc9LSi2C4E1XW7/nk4+FrgCLSsCzA0VPxoJ19kUL5CavMzT7cOa3scAnaRMeNdGsasRSUcLb1WsdiiEsNQqI0sG4LXEPHZ2y5hO81U5X3GN42GVDAmoRdVQijWZ07POkqtiwykbkyninUvtTiAdbFP2tW4v8Jlcz3AEj5DgnDk38SfSa7PqtbU0ji9R937idEwtIGgt3WvXrcAzoFtK7478H/Im+IB5j8bQmELQB6Ms5xM9mv/DS/ADdxVD6Nhk3TFI6cf1zIIP8EwnJWHomhJOUzK7YAO8NvgriS/XjLcFWNr0rDZDqsBl5O09dxKFjLDeiry6nl3AOcoB6ijIY8NfLDEg8uMkUuuEkMusAOawu601mlkrUw== ktdseduuser

```







## 2) 방화벽 정책만들기

* 메뉴 : VPC네트워크 > 방화벽
* 정책명 : allow-ssh-from-ktds-edu
* 소스IP : 0.0.0.0/0    <-- 모든 소스 를 다 허용
* 대상 net : 모든 Instance
* port : 22





## 3) VM 서버 Setup



### (1) 인스턴스 템플릿 생성

> k3s cluster 용도 VM 을 쉽게 생성



##### 템플릿1

* 용도

  * bastion server 용도로 사용했었으나 

  * k3s / strimzi 설치하고 나니 4GB 로는 부족하다.

* 이름 : ktds-k3s-template

* 머신유형 : e2-medium (vCPU 2개, 4GB 메모리)

* 부팅디스크

  * OS : Ubuntu 22.04 LTS 
    * x86/64, amd64 jammy image built on 2023-04-29, supports Shielded VM features
  * disk : 100GB

* 보안

  * VM 액세스

    * 프로젝트 차원 SSH 키 차단 : check

    * ssh key 추가  ( ktdseduuser.pub )

      * ```
        ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQDVhqICvHjqGjvJ5lyedivtpjomZZU20qtO8+KGTWxDtyyxX5ho14fxNSA0T9zukpbvZ0rH0K38nAcRvaCBtZxbWGk4mxshBxBO1xi4gr/Igi81+7s92ByuJuiZVXformhx/5zfma16aIKOdPr0ffUIpjr6ocZQce9RwoR5pGYn3/QioCJSsDNbDE3leWs9ppC28u2PwmMq9ElnjbtHlL0yxAM5uLPVheWaF5e5x4266IptpJNELphTxeFzC2JIy2Sc5V0O/H728Str+ialszP+Ghv7WuwEc/E7eMT7dGflVy+j/5kcLgxREoxxns8JEDDTZ1QORmjOC9fpOuUTZwdFI6ppNscdIbQZ3x9vWEbPrtaJNAOl3gkYyyLiEnzTt7iImC+5NJ0t3qp2TDcrxc56Ig/drLHP1+Wnir9Ga2z3JAvBoMm88tBnBM3XqsO0r8bee59S85h68XYo2NwURPmYeggS/38jeoYKIwi3GAzJx12c450hSIxL21H6kSCLrZFn1YfPXwfVRtr7t4uo4Dpom9Q2AIR71onyu8REGl1umMPa6vO1MCGJf5yZKUxMiNzjqMWFgD/axOGkE1CtW7/5lmfzJoEkUfBHW4hZ3iPLC8q7ij7Rfvsz4eCbEOp8sK4olEwIohLNL0mqyJyzR1h06Zu3gMS/IOcCjmtw+r3qQQ== ktdseduuser
        ```



##### 템플릿2 - master

* 용도 : master node / bastion server

* 이름 : ktds-k3s-template2

* 머신유형 : e2-standard-2(vCPU 2개, 8GB 메모리)

* 부팅디스크

  * OS : Ubuntu 22.04 LTS 
    * x86/64, amd64 jammy image built on 2023-04-29, supports Shielded VM features
  * disk : 100GB

* 보안

  * VM 액세스

    * 프로젝트 차원 SSH 키 차단 : check

    * ssh key 추가  ( ktdseduuser.pub )

      * ```
        ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQDVhqICvHjqGjvJ5lyedivtpjomZZU20qtO8+KGTWxDtyyxX5ho14fxNSA0T9zukpbvZ0rH0K38nAcRvaCBtZxbWGk4mxshBxBO1xi4gr/Igi81+7s92ByuJuiZVXformhx/5zfma16aIKOdPr0ffUIpjr6ocZQce9RwoR5pGYn3/QioCJSsDNbDE3leWs9ppC28u2PwmMq9ElnjbtHlL0yxAM5uLPVheWaF5e5x4266IptpJNELphTxeFzC2JIy2Sc5V0O/H728Str+ialszP+Ghv7WuwEc/E7eMT7dGflVy+j/5kcLgxREoxxns8JEDDTZ1QORmjOC9fpOuUTZwdFI6ppNscdIbQZ3x9vWEbPrtaJNAOl3gkYyyLiEnzTt7iImC+5NJ0t3qp2TDcrxc56Ig/drLHP1+Wnir9Ga2z3JAvBoMm88tBnBM3XqsO0r8bee59S85h68XYo2NwURPmYeggS/38jeoYKIwi3GAzJx12c450hSIxL21H6kSCLrZFn1YfPXwfVRtr7t4uo4Dpom9Q2AIR71onyu8REGl1umMPa6vO1MCGJf5yZKUxMiNzjqMWFgD/axOGkE1CtW7/5lmfzJoEkUfBHW4hZ3iPLC8q7ij7Rfvsz4eCbEOp8sK4olEwIohLNL0mqyJyzR1h06Zu3gMS/IOcCjmtw+r3qQQ== ktdseduuser
        ```





##### 템플릿3 - worker node 용도

* 이름 : ktds-k3s-template-c4m16

* 머신유형 : e2-standard-4(vCPU 4개, 16GB 메모리)

* 부팅디스크

  * OS : Ubuntu 22.04 LTS 
    * x86/64, amd64 jammy image built on 2023-04-29, supports Shielded VM features
  * disk : 100GB

* 보안

  * VM 액세스

    * 프로젝트 차원 SSH 키 차단 : check

    * ssh key 추가  ( ktdseduuser.pub )

      * ```
        ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQDVhqICvHjqGjvJ5lyedivtpjomZZU20qtO8+KGTWxDtyyxX5ho14fxNSA0T9zukpbvZ0rH0K38nAcRvaCBtZxbWGk4mxshBxBO1xi4gr/Igi81+7s92ByuJuiZVXformhx/5zfma16aIKOdPr0ffUIpjr6ocZQce9RwoR5pGYn3/QioCJSsDNbDE3leWs9ppC28u2PwmMq9ElnjbtHlL0yxAM5uLPVheWaF5e5x4266IptpJNELphTxeFzC2JIy2Sc5V0O/H728Str+ialszP+Ghv7WuwEc/E7eMT7dGflVy+j/5kcLgxREoxxns8JEDDTZ1QORmjOC9fpOuUTZwdFI6ppNscdIbQZ3x9vWEbPrtaJNAOl3gkYyyLiEnzTt7iImC+5NJ0t3qp2TDcrxc56Ig/drLHP1+Wnir9Ga2z3JAvBoMm88tBnBM3XqsO0r8bee59S85h68XYo2NwURPmYeggS/38jeoYKIwi3GAzJx12c450hSIxL21H6kSCLrZFn1YfPXwfVRtr7t4uo4Dpom9Q2AIR71onyu8REGl1umMPa6vO1MCGJf5yZKUxMiNzjqMWFgD/axOGkE1CtW7/5lmfzJoEkUfBHW4hZ3iPLC8q7ij7Rfvsz4eCbEOp8sK4olEwIohLNL0mqyJyzR1h06Zu3gMS/IOcCjmtw+r3qQQ== ktdseduuser
        ```





##### 템플릿4 - bastion(교육생)

* 용도 :  bastion server(교육생들)

* 이름 : ktds-k3s-template4

* 머신유형 : e2-standard-2(vCPU 2개, 8GB 메모리)

* 부팅디스크

  * OS : Ubuntu 22.04 LTS
    * x86/64, amd64 jammy image built on 2023-04-29, supports Shielded VM features
  * disk : 100GB

* 보안

  * VM 액세스

    * 프로젝트 차원 SSH 키 차단 : check

    * ssh key 추가  ( ktdseduuser.pub )

      * ```
        ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQDw55OINanLf7jxw4FannWGNqFvhX9yzauCliIvWezzr9Go0PepaQKSeRcA/cKQ9tK7E/AuSTZooHC7iVbDCAcbtv72iycYkpjdk3rU2+u8fWqnZW8ih1JYSJIuREmxPqQZLMi1QbiCBhmKkjkq3YAgaNFI+kMw9LpEIf9dIgNnfqOGzbfyp+KCOrymToZkIQKVCAdtxeN9ea5LHpId2CbO9gJMOrSaQDIcz/9JO6jRjMwGzh6D6DQdnZna02+I/CHcHa3aghjPEhG7jMkrICfowRK5/9S65S6XHy17w0KU7fBGHD5TONV+MUc9LSi2C4E1XW7/nk4+FrgCLSsCzA0VPxoJ19kUL5CavMzT7cOa3scAnaRMeNdGsasRSUcLb1WsdiiEsNQqI0sG4LXEPHZ2y5hO81U5X3GN42GVDAmoRdVQijWZ07POkqtiwykbkyninUvtTiAdbFP2tW4v8Jlcz3AEj5DgnDk38SfSa7PqtbU0ji9R937idEwtIGgt3WvXrcAzoFtK7478H/Im+IB5j8bQmELQB6Ms5xM9mv/DS/ADdxVD6Nhk3TFI6cf1zIIP8EwnJWHomhJOUzK7YAO8NvgriS/XjLcFWNr0rDZDqsBl5O09dxKFjLDeiry6nl3AOcoB6ijIY8NfLDEg8uMkUuuEkMusAOawu601mlkrUw== ktdseduuser
        ```









### (2) VM 생성 : master

인스턴스 이름을 직접 지정하기 위해 개별생성한다. 

* VM인스턴스 개별 생성

  * "템플릿에서 VM인스턴스 만들기" 선택
  * 템플릿 : ktds-k3s-template2
  * 이름 : ktds-k3s-master01, 02 , 03
  * 라벨
    * node: master
  * 리전 : us-central1 (아이오와) / 영역: us-central1-a

  

* 인스턴스 그룹 생성

  * master 는 LB 와 연결 목적으로 생성한다.
  * Status: Unmanaged
    * "New unmanaged instance group" 선택
  * 이름 : ktds-k3s-master-group
  * 구성원 : ktds-k3s-master01, 02 , 03



### (3) VM 생성 : worker

인스턴스 이름을 직접 지정하기 위해 개별생성한다. 

* VM인스턴스 개별 생성

  * "템플릿에서 VM인스턴스 만들기" 선택
  * 템플릿 : ktds-k3s-template-c4m16
  * 이름 : ktds-k3s-worker01, 02 , 03, 04, 05
  * 라벨
    * node: worker
  * 리전
    * 리전이 다를수 도 있다.  리전이 다르면 그룹으로 묶을 수 없다. (worker node 는 굳이 그룹을 묶을 필요 없다.)
    * worker01
      * us-central1 (아이오와) / 영역: us-central1-a
    * worker02, 03
      * 중동 - me-west1 (텔아비브) / 영역: me-west1-a
    * worker04
      * 중동 - me-west1 (텔아비브) / 영역: me-west1-a




* 인스턴스 그룹 생성

  * worker 는 별도 인스턴스 그룹을 생성하지 않아도 된다.
  * Status: Unmanaged
    * "New unmanaged instance group" 선택
  * 이름 : ktds-k3s-worker-group
  * 위치 : us-central1(아이오와) / us-central1-a
  * 구성원 : ktds-k3s-master01, 02 , 03



### (4) VM 생성 : bastion01

인스턴스 이름을 직접 지정하기 위해 개별생성한다. 

* VM인스턴스 개별 생성

  * "템플릿에서 VM인스턴스 만들기" 선택
  * 템플릿 : ktds-k3s-template2
  * 이름 : ktds-bastion01, 02
  * 라벨
    * node: bastion
  * 리전 :  southamerica-east1-b




* 인스턴스 그룹 생성

  * bastion 은 그룹생성 불필



### (5) VM 생성 : bastion03

인스턴스 이름을 직접 지정하기 위해 개별생성한다. 

* VM인스턴스 개별 생성

  * "템플릿에서 VM인스턴스 만들기" 선택
  * 템플릿 : ktds-k3s-template2
  * 이름 : bastion02 ~ 16
  * 라벨
    * node: bastion
  * 리전
    * 리전 :  southamerica-east1-b (상파울루)   <-- 선택
    * 리전 : us-central1 (아이오와)
      * 영역: us-central1-a




* 인스턴스 그룹 생성

  * bastion 은 그룹생성 불필





### (6) 머신 이미지 생성





[bastion-image-from-03](https://console.cloud.google.com/compute/machineImages/details/bastion-image-from-03?project=ktds-architecture)



#### 머신 이미지1 - 삭제됨

* 머신이미지 만들기

  * bastion03 을 기준으로 image 를 만들자.
  * 이름 : bastion-image-from-03
  * 설명
    * 교육생 전용 bastion
    * 공용 sshkey ( admin 과 같이 사용함 ) - 지금은 사용하지 말자.
    * docker, helm

  * 소스 VM 인스턴스 : bastion03
  * 위치
    * 멀티리전 : us



#### 머신 이미지2 - 삭제됨

* 머신이미지 만들기
  * bastion04 을 기준으로 image 를 만들자.
  * 이름 : bastion-image
  * 설명
    * 교육생 전용 bastion
    * 전용 sshkey
    * docker, helm
    * 용도 : k8s, istio 교육때 사용
  
  * 소스 VM 인스턴스 : bastion04
  * 위치
    * 멀티리전 : us



#### 머신 이미지3 - k3s

* 머신이미지 만들기

  * bastion02 을 기준으로 image 를 만들자.

  * 이름 : bastion-image-k3s

  * 설명
    * 교육생 전용 bastion
    * 전용 sshkey
    * docker, helm
    * k3s
    * 용도 : kafka, redis 교육때 사용

  * 소스 VM 인스턴스 : bastion02

  * 위치
    * 멀티리전 : us





#### 머신 이미지4 - k3s-v2

* 머신이미지 만들기

  * bastion05 을 기준으로 image 를 만들자.

  * 이름 : bastion-image-k3s-v2

  * 설명
    * 교육생 전용 bastion
    * 전용 sshkey
    * docker, helm
    * k3s
    * 용도 : kafka, redis 교육때 사용

  * 소스 VM 인스턴스 : bastion05

  * 위치
    * 멀티리전 : us
    * 

이미지 생성 작업시 주의할점 - ★★★

```sh

# 1) 불필요한 directry 삭제할것
$ rm -rf ~/temp/strimzi           # <-- strimzi directory 삭제
  rm -rf ~/temp/helm
  rm -rf ~/githubrepo/ktds-edu-k8s-istio
  rm -rf ~/del/


# 2) docker rm
$ docker rm -f kafkacat
$ docker rm -f python


# 3) git clone
$ cd ~/githubrepo
  git clone https://github.com/ssongman/ktds-edu-kafka-redis.git

```













### (7) VM 생성 : bastion02~20

* VM인스턴스 개별 생성
  * "머신 이미지에서 VM 인스턴스 만들기" 선택
  
  * 이미지선택 : bastion-image-k3s
  
  * 이름 : bastion02 ~ 16
  
  * 라벨
    * node: bastion
    
  * 리전
    
    * k8s/istio 교육
    
      * southamerica-east1-b (상파울루)          <-- 선택
      * northamerica-northeast2 (토론토) / northamerica-northeast2-a
      * us-central1(아이오와)                              <-- 가장 저렴하지만 리소스가 없다.
      * northamerica-northeast1 (몬트리올)/northamerica-northeast1-a
      * us-west1 (오리건) / us-west1-b
      * europe-north1 (핀란드) /europe-north1-a
    
    * kafka/redis 교육
    
      * asia-east1 (타이완)/asia-east1-b
    
      * asia-south1 (뭄바이)/asia-south1-c
    
      * asia-northeast3 (서울)/asia-northeast3-a
    
      * asia-northeast2 (오사카)/asia-northeast2-a
    
        
    
  




## 4) 기타 설정 및 연결테스트



### (1) VM 목록

*  vm 목록

| 이름                                                         | 영역                 | 내부 IP                                                      | 외부 IP                                                      | 연결 |
| ------------------------------------------------------------ | -------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ---- |
| [ktds-k3s-master01](https://console.cloud.google.com/compute/instancesDetail/zones/us-central1-a/instances/ktds-k3s-master01?project=ktds-architecture) | us-central1-a        | 10.128.0.25 ([nic0](https://console.cloud.google.com/networking/networkinterfaces/zones/us-central1-a/instances/ktds-k3s-master01?networkInterface=nic0&project=ktds-architecture)) | 35.202.110.75 ([nic0](https://console.cloud.google.com/networking/networkinterfaces/zones/us-central1-a/instances/ktds-k3s-master01?networkInterface=nic0&project=ktds-architecture)) | SSH  |
| [ktds-k3s-master02](https://console.cloud.google.com/compute/instancesDetail/zones/us-central1-a/instances/ktds-k3s-master02?project=ktds-architecture) | us-central1-a        | 10.128.0.26 ([nic0](https://console.cloud.google.com/networking/networkinterfaces/zones/us-central1-a/instances/ktds-k3s-master02?networkInterface=nic0&project=ktds-architecture)) | 35.224.158.79 ([nic0](https://console.cloud.google.com/networking/networkinterfaces/zones/us-central1-a/instances/ktds-k3s-master02?networkInterface=nic0&project=ktds-architecture)) | SSH  |
| [ktds-k3s-master03](https://console.cloud.google.com/compute/instancesDetail/zones/us-central1-a/instances/ktds-k3s-master03?project=ktds-architecture) | us-central1-a        | 10.128.0.27 ([nic0](https://console.cloud.google.com/networking/networkinterfaces/zones/us-central1-a/instances/ktds-k3s-master03?networkInterface=nic0&project=ktds-architecture)) | 34.136.12.122 ([nic0](https://console.cloud.google.com/networking/networkinterfaces/zones/us-central1-a/instances/ktds-k3s-master03?networkInterface=nic0&project=ktds-architecture)) | SSH  |
| [ktds-k3s-worker01](https://console.cloud.google.com/compute/instancesDetail/zones/us-central1-a/instances/ktds-k3s-worker01?project=ktds-architecture) | us-central1-a        | 10.128.0.28 ([nic0](https://console.cloud.google.com/networking/networkinterfaces/zones/us-central1-a/instances/ktds-k3s-worker01?networkInterface=nic0&project=ktds-architecture)) | 34.29.55.136 ([nic0](https://console.cloud.google.com/networking/networkinterfaces/zones/us-central1-a/instances/ktds-k3s-worker01?networkInterface=nic0&project=ktds-architecture)) | SSH  |
| [ktds-k3s-worker02](https://console.cloud.google.com/compute/instancesDetail/zones/us-central1-a/instances/ktds-k3s-worker02?project=ktds-architecture) | us-central1-a        | 10.128.0.29 ([nic0](https://console.cloud.google.com/networking/networkinterfaces/zones/us-central1-a/instances/ktds-k3s-worker02?networkInterface=nic0&project=ktds-architecture)) | 34.172.123.121 ([nic0](https://console.cloud.google.com/networking/networkinterfaces/zones/us-central1-a/instances/ktds-k3s-worker02?networkInterface=nic0&project=ktds-architecture)) | SSH  |
| [ktds-k3s-worker03](https://console.cloud.google.com/compute/instancesDetail/zones/southamerica-east1-b/instances/ktds-k3s-worker03?project=ktds-architecture) | southamerica-east1-b | 10.158.0.25 ([nic0](https://console.cloud.google.com/networking/networkinterfaces/zones/southamerica-east1-b/instances/ktds-k3s-worker03?networkInterface=nic0&project=ktds-architecture)) | 35.247.252.172 ([nic0](https://console.cloud.google.com/networking/networkinterfaces/zones/southamerica-east1-b/instances/ktds-k3s-worker03?networkInterface=nic0&project=ktds-architecture)) | SSH  |
| [ktds-bastion01](https://console.cloud.google.com/compute/instancesDetail/zones/southamerica-east1-b/instances/ktds-bastion01?project=ktds-architecture) | southamerica-east1-b | 10.158.0.26 ([nic0](https://console.cloud.google.com/networking/networkinterfaces/zones/southamerica-east1-b/instances/ktds-bastion01?networkInterface=nic0&project=ktds-architecture)) | 34.151.244.163 ([nic0](https://console.cloud.google.com/networking/networkinterfaces/zones/southamerica-east1-b/instances/ktds-bastion01?networkInterface=nic0&project=ktds-architecture)) | SSH  |



```


교육생 : 총 14명

ㅇ bastion server 갯수
- 강사 : 2대
- 교육생 : 14대

ktds-bastion01 : 송양종
ktds-bastion02 : 사용자용 - 송양종(edu-user03)
ktds-bastion03 : 사용자용 - edu-user03
ktds-bastion04 : 사용자용 - edu-user04
ktds-bastion05 : 사용자용 - edu-user05
...
ktds-bastion14 : 사용자용 - edu-user14
ktds-bastion15 : 사용자용 - edu-user15
ktds-bastion16 : 사용자용 - edu-user16



ㅇ 개인 bastion-server 용도
- k3s 설치
- strimzi 설치
- redis Cluster 설치
- redis 설치

ㅇ 리소스
4core에 8GB 정도로 할당

```










### (2) 연결 테스트

```sh
# key file이 존재하는 위치로 
$ cd /root/song/gcp/ktds-k3s

# master01
$ ssh -i ktdseduuser ktdseduuser@34.105.107.95

# master02
$ ssh -i ktdseduuser ktdseduuser@35.230.121.203

# master03
$ ssh -i ktdseduuser ktdseduuser@35.230.73.235

# worker01
$ ssh -i ktdseduuser ktdseduuser@35.247.229.18

# worker02
$ ssh -i ktdseduuser ktdseduuser@34.151.209.77

# worker03
$ ssh -i ktdseduuser ktdseduuser@34.151.244.163

# worker04
$ ssh -i ktdseduuser ktdseduuser@35.198.22.131

# worker05
$ ssh -i ktdseduuser ktdseduuser@35.247.252.172

# worker05
$ ssh -i ktdseduuser ktdseduuser@35.247.252.172

# bastion01
$ ssh -i ktdseduuser ktdseduuser@34.27.41.215


```



### (3) root password 등록

```sh
# 아래 입력하면 변경 가능
$ sudo passwd
New password: r******s
Retype new password: r******s


# root 로 변경
$ su
Password:

```





# 2. K3S Setup





## 1) K3S 구성(HA mode)

### (1) master node

```sh
# root 권한으로 수행
$ su


# master01에서
$ curl -sfL https://get.k3s.io | sh -s - --write-kubeconfig-mode 644 --cluster-init

# 확인
$ kubectl version
Client Version: version.Info{Major:"1", Minor:"26", GitVersion:"v1.26.4+k3s1", GitCommit:"8d0255af07e95b841952563253d27b0d10bd72f0", GitTreeState:"clean", BuildDate:"2023-04-20T00:33:18Z", GoVersion:"go1.19.8", Compiler:"gc", Platform:"linux/amd64"}
Kustomize Version: v4.5.7
Server Version: version.Info{Major:"1", Minor:"26", GitVersion:"v1.26.4+k3s1", GitCommit:"8d0255af07e95b841952563253d27b0d10bd72f0", GitTreeState:"clean", BuildDate:"2023-04-20T00:33:18Z", GoVersion:"go1.19.8", Compiler:"gc", Platform:"linux/amd64"}
---
WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.
Client Version: version.Info{Major:"1", Minor:"26", GitVersion:"v1.26.4+k3s1", GitCommit:"8d0255af07e95b841952563253d27b0d10bd72f0", GitTreeState:"clean", BuildDate:"2023-04-20T00:33:18Z", GoVersion:"go1.19.8", Compiler:"gc", Platform:"linux/amd64"}
Kustomize Version: v4.5.7
Server Version: version.Info{Major:"1", Minor:"26", GitVersion:"v1.26.4+k3s1", GitCommit:"8d0255af07e95b841952563253d27b0d10bd72f0", GitTreeState:"clean", BuildDate:"2023-04-20T00:33:18Z", GoVersion:"go1.19.8", Compiler:"gc", Platform:"linux/amd64"}





# IP/ token 확인
$ cat /var/lib/rancher/k3s/server/node-token
K1053500b81d3608e011e82c285ad06590811db8059cca5ca0b3cbd10e969f95e03::server:9bd15fea5819e2ce185669abd5a1ce48
---
K10b0a3c6320b9ceff2b14ea265774c388e7aa4ad8cfa35804efb98eb9235f99290::server:5e7fd6a284e028bdcba918abd9fae74e




# master01 에서
$ kubectl get nodes -w

# master02, 03 에서
$ export MASTER_TOKEN="K10b0a3c6320b9ceff2b14ea265774c388e7aa4ad8cfa35804efb98eb9235f99290::server:5e7fd6a284e028bdcba918abd9fae74e"
  export MASTER_IP="10.128.0.35"

$ curl -sfL https://get.k3s.io | sh -s - --write-kubeconfig-mode 644 --server https://${MASTER_IP}:6443 --token ${MASTER_TOKEN}

…
[INFO]  systemd: Starting k3s-agent   ← 정상 로그




# master01 에서
$ kubectl get nodes
NAME                STATUS   ROLES                       AGE     VERSION
ktds-k3s-master01   Ready    control-plane,etcd,master   2m30s   v1.26.4+k3s1
ktds-k3s-master02   Ready    control-plane,etcd,master   29s     v1.26.4+k3s1
ktds-k3s-master03   Ready    control-plane,etcd,master   46s     v1.26.4+k3s1



# [참고]istio setup을 위한 k3s 설정시 아래 참고
## traefik 을 deploy 하지 않는다. 
## istio 에서 별도 traefic 을 설치하는데 이때 기설치된 controller 가 있으면 충돌 발생함 - istio version 1.14
$ curl -sfL https://get.k3s.io |INSTALL_K3S_EXEC="--no-deploy traefik" sh -

# [참고] istio version 1.14 에서 현재 1.17.2로 버전업되면서 DaemonSet 이 사라졌다.
# traefik 과 충돌 현상도 사라졌다.
```





#### [참고] 수동실행

설치가 안된다면 아래와 같이 수동실행 진행해 보자.

```sh
# root 권한으로

$ export MASTER_TOKEN="K10b0a3c6320b9ceff2b14ea265774c388e7aa4ad8cfa35804efb98eb9235f99290::server:5e7fd6a284e028bdcba918abd9fae74e"
  export MASTER_IP="10.128.0.35"


$ sudo k3s server --server https://${MASTER_IP}:6443 --token ${MASTER_TOKEN} &


$ k3s server &
…
COMMIT 
…

# k3s 데몬 확인
$ ps -ef|grep k3s
root         590     405  0 13:05 pts/0    00:00:00 sudo k3s server
root         591     590 76 13:05 pts/0    00:00:26 k3s server
root         626     591  5 13:05 pts/0    00:00:01 containerd -c /var/lib/rancher/k3s/agent/etc/containerd/config.toml -a /run/k3s/containerd/containerd.sock --state /run/k3s/containerd --root /var/lib/rancher/k3s/agent/containerd
...

$ k3s kubectl version
WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.
Client Version: version.Info{Major:"1", Minor:"26", GitVersion:"v1.26.4+k3s1", GitCommit:"8d0255af07e95b841952563253d27b0d10bd72f0", GitTreeState:"clean", BuildDate:"2023-04-20T00:33:18Z", GoVersion:"go1.19.8", Compiler:"gc", Platform:"linux/amd64"}
Kustomize Version: v4.5.7
Server Version: version.Info{Major:"1", Minor:"26", GitVersion:"v1.26.4+k3s1", GitCommit:"8d0255af07e95b841952563253d27b0d10bd72f0", GitTreeState:"clean", BuildDate:"2023-04-20T00:33:18Z", GoVersion:"go1.19.8", Compiler:"gc", Platform:"linux/amd64"}

```







#### [참고] Master Node 비정상일 경우

master node 중 하나가 비정상 작동한다면 아래와 같이 수행하자.

* 시나리오1
  * 가정
    * master1번이 비정상, master2,3 은 정상일 경우
  * 조치
    * token은 최초 공유했던 token 값으로 유지한다.
    * master_ip 는 정상인 2번의 IP 를 참고한다.

```sh
# root 권한으로

# 1) k3s kill
$ sh /usr/local/bin/k3s-killall.sh

# 2) 환경변수 셋팅
# 정상인 MASTER_IP 를 셋팅한다.
$ export MASTER_TOKEN="K10b0a3c6320b9ceff2b14ea265774c388e7aa4ad8cfa35804efb98eb9235f99290::server:5e7fd6a284e028bdcba918abd9fae74e"
  export MASTER_IP="10.128.0.35"

# 3) k3s 실행
$ sudo k3s server --server https://${MASTER_IP}:6443 --token ${MASTER_TOKEN} &

…
COMMIT 
…

# 4) k3s 데몬 확인
$ ps -ef|grep k3s
root         590     405  0 13:05 pts/0    00:00:00 sudo k3s server
root         591     590 76 13:05 pts/0    00:00:26 k3s server
root         626     591  5 13:05 pts/0    00:00:01 containerd -c /var/lib/rancher/k3s/agent/etc/containerd/config.toml -a /run/k3s/containerd/containerd.sock --state /run/k3s/containerd --root /var/lib/rancher/k3s/agent/containerd
...

# 5) kubectl version 확인
$ k3s kubectl version
WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.
Client Version: version.Info{Major:"1", Minor:"26", GitVersion:"v1.26.4+k3s1", GitCommit:"8d0255af07e95b841952563253d27b0d10bd72f0", GitTreeState:"clean", BuildDate:"2023-04-20T00:33:18Z", GoVersion:"go1.19.8", Compiler:"gc", Platform:"linux/amd64"}
Kustomize Version: v4.5.7
Server Version: version.Info{Major:"1", Minor:"26", GitVersion:"v1.26.4+k3s1", GitCommit:"8d0255af07e95b841952563253d27b0d10bd72f0", GitTreeState:"clean", BuildDate:"2023-04-20T00:33:18Z", GoVersion:"go1.19.8", Compiler:"gc", Platform:"linux/amd64"}

```











### (2) worker node

```sh
# root 권한으로 수행
# worker node 에서 각각 수행


$ export MASTER_TOKEN="K10b0a3c6320b9ceff2b14ea265774c388e7aa4ad8cfa35804efb98eb9235f99290::server:5e7fd6a284e028bdcba918abd9fae74e"
  export MASTER_IP="10.128.0.35"
  

$ curl -sfL https://get.k3s.io | K3S_URL=https://${MASTER_IP}:6443 K3S_TOKEN=${MASTER_TOKEN} sh -

…
[INFO]  systemd: Starting k3s-agent   ← 나오면 정상



# master01 에서
$ kubectl get nodes
NAME                STATUS   ROLES                       AGE     VERSION
ktds-k3s-master01   Ready    control-plane,etcd,master   5m19s   v1.26.4+k3s1
ktds-k3s-master02   Ready    control-plane,etcd,master   3m18s   v1.26.4+k3s1
ktds-k3s-master03   Ready    control-plane,etcd,master   3m35s   v1.26.4+k3s1
ktds-k3s-worker01   Ready    <none>                      28s     v1.26.4+k3s1
ktds-k3s-worker02   Ready    <none>                      63s     v1.26.4+k3s1
ktds-k3s-worker03   Ready    <none>                      53s     v1.26.4+k3s1


```



#### [참고] 수동실행

설치가 안된다면 아래와 같이 수동실행 진행해 보자.

```sh
# root 권한으로

$ sudo k3s agent --server https://${MASTER_IP}:6443 --token ${NODE_TOKEN} &
```









### (3) Clean up

#### node 제거

##### Cordon

현재 노드에 배포된 Pod은 그대로 유지하면서, 추가적인 Pod의 배포를 제한하는 명령어

```
$ kubectl uncordon <node-name>

$ kubectl cordon <node-name>
```

`kubectl drain` 혹은 `kubectl cordon` 명령어를 적용한 노드는 `SechedulingDisabled` 상태가 되어 더 이상 Pod이 scheduling되지 않는다.`kubectl uncordon`은 노드의 이러한 `SchedulingDisabled` 상태를 제거하여 노드에 Pod이 정상적으로 스케쥴링 될 수 있도록 복구하는 명령어이다.



##### Drain

노드에 존재하는 모든 Pod을 제거하여 노드를 비우고, Pod들을 다른 노드에 새롭게 스케쥴링하는 명령어
drain 과정에 cordon이 포함되어 있다고 볼 수 있다.

```sh
$ kubectl get nodes
NAME                STATUS   ROLES                       AGE    VERSION
ktds-k3s-master01   Ready    control-plane,etcd,master   100d   v1.26.4+k3s1
ktds-k3s-master02   Ready    control-plane,etcd,master   100d   v1.26.4+k3s1
ktds-k3s-master03   Ready    control-plane,etcd,master   86d    v1.26.5+k3s1
ktds-k3s-worker01   Ready    <none>                      100d   v1.26.4+k3s1
ktds-k3s-worker02   Ready    <none>                      86d    v1.26.5+k3s1
ktds-k3s-worker03   Ready    <none>                      86d    v1.26.5+k3s1
ktds-k3s-worker04   Ready    <none>                      21h    v1.27.4+k3s1




$ kubectl drain ktds-k3s-worker04 --ignore-daemonsets --delete-emptydir-data
$ kubectl drain ktds-k3s-worker03 --ignore-daemonsets --delete-emptydir-data
$ kubectl drain ktds-k3s-worker02 --ignore-daemonsets --delete-emptydir-data
$ kubectl drain ktds-k3s-worker01 --ignore-daemonsets --delete-emptydir-data





$ kubectl get nodes
NAME                STATUS                     ROLES                       AGE     VERSION
ktds-k3s-master01   Ready                      control-plane,etcd,master   64m     v1.26.4+k3s1
ktds-k3s-master02   Ready                      control-plane,etcd,master   17m     v1.26.4+k3s1
ktds-k3s-master03   Ready                      control-plane,etcd,master   9m37s   v1.26.4+k3s1
ktds-k3s-worker01   Ready                      control-plane,etcd,master   7m51s   v1.26.4+k3s1
ktds-k3s-worker02   Ready                      control-plane,etcd,master   7m37s   v1.26.4+k3s1
ktds-k3s-worker03   Ready                      control-plane,etcd,master   7m37s   v1.26.4+k3s1
ktds-k3s-worker04   Ready,SchedulingDisabled   control-plane,etcd,master   21h    v1.27.4+k3s1


```



##### Node Delete

```sh
# drain 작업 이후
$ kubectl delete node ktds-k3s-worker04
$ kubectl delete node ktds-k3s-worker03
$ kubectl delete node ktds-k3s-worker02
$ kubectl delete node ktds-k3s-worker01

```



#### k3s uninstall

```sh
# root권한으로

## uninstall
$ sh /usr/local/bin/k3s-killall.sh
  sh /usr/local/bin/k3s-uninstall.sh
  
```







## 2) K3S 구성(Single mode)

### (1) master node

```sh
# root 권한으로 수행
$ su

$ curl -sfL https://get.k3s.io | sh -s - --write-kubeconfig-mode 644

or

$ curl -sfL https://get.k3s.io | sh -

# 확인
$ kubectl version
WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.
Client Version: version.Info{Major:"1", Minor:"26", GitVersion:"v1.26.5+k3s1", GitCommit:"7cefebeaac7dbdd0bfec131ea7a43a45cb125354", GitTreeState:"clean", BuildDate:"2023-05-27T00:05:40Z", GoVersion:"go1.19.9", Compiler:"gc", Platform:"linux/amd64"}
Kustomize Version: v4.5.7
Server Version: version.Info{Major:"1", Minor:"26", GitVersion:"v1.26.5+k3s1", GitCommit:"7cefebeaac7dbdd0bfec131ea7a43a45cb125354", GitTreeState:"clean", BuildDate:"2023-05-27T00:05:40Z", GoVersion:"go1.19.9", Compiler:"gc", Platform:"linux/amd64"}


# IP/ token 확인
$ cat /var/lib/rancher/k3s/server/node-token
K10f74ce1e1f309271e78114c63d51d5936249e3d379faf1c5c7b2269218f2f9220::server:459b5947077d6e612074e998ff769dd8


# 확인
$ kubectl get nodes -o wide
NAME        STATUS   ROLES                  AGE   VERSION        INTERNAL-IP   EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION    CONTAINER-RUNTIME
bastion03   Ready    control-plane,master   47s   v1.26.5+k3s1   10.158.0.29   <none>        Ubuntu 22.04.2 LTS   5.19.0-1022-gcp   containerd://1.7.1-k3s1


```







## 3) 기타 설정

### (1) kubeconfig 설정

일반 User가 직접 kubctl 명령 실행을 위해서는 kube config 정보(~/.kube/config) 가 필요하다.

k3s 를 설치하면 /etc/rancher/k3s/k3s.yaml 에 정보가 존재하므로 이를 복사한다. 또한 모든 사용자가 읽을 수 있도록 권한을 부여 한다.

```sh
## root 로 실행
$ su

$ ll /etc/rancher/k3s/k3s.yaml
-rw------- 1 root root 2961 May 14 03:23 /etc/rancher/k3s/k3s.yaml

# 모든 사용자에게 읽기권한 부여
$ chmod +r /etc/rancher/k3s/k3s.yaml

$ ll /etc/rancher/k3s/k3s.yaml
-rw-r--r-- 1 root root 2961 May 14 03:23 /etc/rancher/k3s/k3s.yaml

# 일반 user 로 전환
$ exit




## 사용자 권한으로 실행

$ mkdir -p ~/.kube

$ cp /etc/rancher/k3s/k3s.yaml ~/.kube/config

$ ll ~/.kube/config
-rw-r--r-- 1 song song 2957 May 14 03:44 /home/song/.kube/config

# 자신만 RW 권한 부여
$ chmod 600 ~/.kube/config

$ ls -ltr ~/.kube/config
-rw------- 1 ktdseduuser ktdseduuser 2957 May 13 14:35 /home/ktdseduuser/.kube/config



## 확인
$ kubectl version
Client Version: version.Info{Major:"1", Minor:"26", GitVersion:"v1.26.5+k3s1", GitCommit:"7cefebeaac7dbdd0bfec131ea7a43a45cb125354", GitTreeState:"clean", BuildDate:"2023-05-27T00:05:40Z", GoVersion:"go1.19.9", Compiler:"gc", Platform:"linux/amd64"}
Kustomize Version: v4.5.7
Server Version: version.Info{Major:"1", Minor:"26", GitVersion:"v1.26.5+k3s1", GitCommit:"7cefebeaac7dbdd0bfec131ea7a43a45cb125354", GitTreeState:"clean", BuildDate:"2023-05-27T00:05:40Z", GoVersion:"go1.19.9", Compiler:"gc", Platform:"linux/amd64"}

```

root 권한자가 아닌 다른 사용자도 사용하려면 위와 동일하게 수행해야한다.



## 4) bastion server Setup

kubectl 명령을 입력할 수 있는 별도 서버를 구성하자.



### (1) kubectl Setup

#### Download the latest release

```sh
# 임시 directory 생성 및 이동
$ mkdir -p /root/song/kubectl
$ cd /root/song/kubectl

# download
$ curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"

$ ll
-rw-r--r-- 1 root root 49246208 May 15 12:25 kubectl

```



#### Validate the binary

```sh
# validate file download
$ curl -LO "https://dl.k8s.io/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl.sha256"

$ ll
-rw-r--r-- 1 root root 49246208 May 15 12:25 kubectl
-rw-r--r-- 1 root root       64 May 15 12:25 kubectl.sha256

# check
$ echo "$(cat kubectl.sha256)  kubectl" | sha256sum --check
kubectl: OK


# 실패일 경우 아래와 같이 ...
# kubectl: FAILED
# sha256sum: WARNING: 1 computed checksum did NOT match
```



#### Install kubectl

```sh
# install
$ sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl

# 확인
$ ll /usr/local/bin
-rwxr-xr-x  1 root root 49246208 May 15 12:29 kubectl*


# version 확인
$ kubectl version --client
WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.
Client Version: version.Info{Major:"1", Minor:"27", GitVersion:"v1.27.1", GitCommit:"4c9411232e10168d7b050c49a1b59f6df9d7ea4b", GitTreeState:"clean", BuildDate:"2023-04-14T13:21:19Z", GoVersion:"go1.20.3", Compiler:"gc", Platform:"linux/amd64"}

```







### (2) kubeconfig 설정

master01 에서 사용하는 config 내용을 가져와서 bastion config 으로 Copy한다.

이때 server 의 IP 는 bastion 서버가 인식가능한 bastion 서버 IP 가 되어야 한다.



#### master01 서버의 config 정보 확인

```sh
$ cat /etc/rancher/k3s/k3s.yaml
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJlRENDQVIyZ0F3SUJBZ0lCQURBS0JnZ3Foa2pPUFFRREFqQWpNU0V3SHdZRFZRUUREQmhyTTNNdGMyVnkKZG1WeUxXTmhRREUyT0RRMU5UTXlOelV3SGhjTk1qTXdOVEl3TURNeU56VTFXaGNOTXpNd05URTNNRE15TnpVMQpXakFqTVNFd0h3WURWUVFEREJock0zTXRjMlZ5ZG1WeUxXTmhRREUyT0RRMU5UTXlOelV3V1RBVEJnY3Foa2pPClBRSUJCZ2dxaGtqT1BRTUJCd05DQUFRQXRvN3U2bTB2WnF6R1RnNjgyMEorek5WdlRBLy9WV1JHbkkwZDBMaVQKd1dmbEtCTzdXa3dLSkNEUGY2U3NyVTMvaXliYzNFTU1WRllJa0Mrc1REU0pvMEl3UURBT0JnTlZIUThCQWY4RQpCQU1DQXFRd0R3WURWUjBUQVFIL0JBVXdBd0VCL3pBZEJnTlZIUTRFRmdRVUJoZmI1SnNIY3BVQnhtQndTOFJTCkh1aWhxMEF3Q2dZSUtvWkl6ajBFQXdJRFNRQXdSZ0loQU10TUwxU2hOaXEySzNudjlRWGl2NGpOUWVVUkV1eWUKVlhoblkwOXZyM29RQWlFQXlrSmZTYlYzeDF1UU1uVGZpSWhZYm41RWdYMTJwNVRvWHk0d0hHclNnU2M9Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K
    server: https://10.128.0.35:6443
  name: default
contexts:
- context:
    cluster: default
    user: default
  name: default
current-context: default
kind: Config
preferences: {}
users:
- name: default
  user:
    client-certificate-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJrakNDQVRlZ0F3SUJBZ0lJTmlWL2RNOEZiZjB3Q2dZSUtvWkl6ajBFQXdJd0l6RWhNQjhHQTFVRUF3d1kKYXpOekxXTnNhV1Z1ZEMxallVQXhOamcwTlRVek1qYzFNQjRYRFRJek1EVXlNREF6TWpjMU5Wb1hEVEkwTURVeApPVEF6TWpjMU5Wb3dNREVYTUJVR0ExVUVDaE1PYzNsemRHVnRPbTFoYzNSbGNuTXhGVEFUQmdOVkJBTVRESE41CmMzUmxiVHBoWkcxcGJqQlpNQk1HQnlxR1NNNDlBZ0VHQ0NxR1NNNDlBd0VIQTBJQUJQYWN1a3VmL0tEcFkrVTQKYUR1TjlVTU15M3diRUpLTzFXemdKYlU4M3UrK3JBMmhZcFFBVXlQdmhnZzA2a2VuTDVOZkVhdy80VHlGVGtaSApubG5WeDh1alNEQkdNQTRHQTFVZER3RUIvd1FFQXdJRm9EQVRCZ05WSFNVRUREQUtCZ2dyQmdFRkJRY0RBakFmCkJnTlZIU01FR0RBV2dCUVRCSC9hK1JBK0dDV0FSKzg3K2pVUE5LMkFPVEFLQmdncWhrak9QUVFEQWdOSkFEQkcKQWlFQTRGd2VOTy9GLytpUXJwemI3VTZZOVJqMTRsSFhrbTlpMmNjN2g3TVBWRHdDSVFDdk9nak5TaXpac3BXKwphRlNqUDAxRUY2RHVCdE5NUGYyOXZ0cUlRZ1hKR1E9PQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCi0tLS0tQkVHSU4gQ0VSVElGSUNBVEUtLS0tLQpNSUlCZHpDQ0FSMmdBd0lCQWdJQkFEQUtCZ2dxaGtqT1BRUURBakFqTVNFd0h3WURWUVFEREJock0zTXRZMnhwClpXNTBMV05oUURFMk9EUTFOVE15TnpVd0hoY05Nak13TlRJd01ETXlOelUxV2hjTk16TXdOVEUzTURNeU56VTEKV2pBak1TRXdId1lEVlFRRERCaHJNM010WTJ4cFpXNTBMV05oUURFMk9EUTFOVE15TnpVd1dUQVRCZ2NxaGtqTwpQUUlCQmdncWhrak9QUU1CQndOQ0FBUXZFWENVVVNmWDlZSTZvd3hwNU9yaTRCK0xQTi92RCt2YmhsbDB5ZjhICkc3SnJLN3FibDhUS3NGNHVxS1NlMXZobnRwc0FySXVrZXZCMTE0WURmTW4wbzBJd1FEQU9CZ05WSFE4QkFmOEUKQkFNQ0FxUXdEd1lEVlIwVEFRSC9CQVV3QXdFQi96QWRCZ05WSFE0RUZnUVVFd1IvMnZrUVBoZ2xnRWZ2Ty9vMQpEelN0Z0Rrd0NnWUlLb1pJemowRUF3SURTQUF3UlFJZ0djdFY5dk1sRGpGcUtFb0NRdktGTDdDblh2Z1BkTUN6CkdBU2tuenlTYldrQ0lRQy8rc0l4S2pGSkpzeUpxeVlUcEZyVGwrNytWZ2xEYlNzZmRadktxZ0xyOWc9PQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==
    client-key-data: LS0tLS1CRUdJTiBFQyBQUklWQVRFIEtFWS0tLS0tCk1IY0NBUUVFSURRaW8xZm83S0xBaEVXU1k5bjM3ckNWNFY5MFkvbm1Mbk5xeXFPUFFPNkRvQW9HQ0NxR1NNNDkKQXdFSG9VUURRZ0FFOXB5NlM1LzhvT2xqNVRob080MzFRd3pMZkJzUWtvN1ZiT0FsdFR6ZTc3NnNEYUZpbEFCVApJKytHQ0RUcVI2Y3ZrMThSckQvaFBJVk9Sa2VlV2RYSHl3PT0KLS0tLS1FTkQgRUMgUFJJVkFURSBLRVktLS0tLQo=
```

`server: https://127.0.0.1:6443`    로 되어 있는데 이를 master01 주소로 변경하여 bastion 서버에 Copy 해야 한다.

`server: https://10.128.0.25:6443`    이렇게 변경



#### bastion 서버 config 설정

```sh
# 일반 사용자로 설정

$ mkdir -p ~/.kube
$ cd ~/.kube


$ cat > config
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJkekNDQVIyZ0F3SUJBZ0lCQURBS0JnZ3Foa2pPUFFRREFqQWpNU0V3SHdZRFZRUUREQmhyTTNNdGMyVnkKZG1WeUxXTmhRREUyT0RReE5USTBOREV3SGhjTk1qTXdOVEUxTVRJd056SXhXaGNOTXpNd05URXlNVEl3TnpJeApXakFqTVNFd0h3WURWUVFEREJock0zTXRjMlZ5ZG1WeUxXTmhRREUyT0RReE5USTBOREV3V1RBVEJnY3Foa2pPClBRSUJCZ2dxaGtqT1BRTUJCd05DQUFUWkw5dkM0dm1wMHRxbzc2bFM0UmhIU3pwbVU5ZVFpT0lwMC9ZbXNhelQKZDZHMDFsVmo4bTFXS2VZUU9BS2oxOWpOSUtuL1BCK05mOWc0TEpqdC95QzJvMEl3UURBT0JnTlZIUThCQWY4RQpCQU1DQXFRd0R3WURWUjBUQVFIL0JBVXdBd0VCL3pBZEJnTlZIUTRFRmdRVXdRVmlTbzhQWTQ3MFlQNmtMSzh2CmVPbmgrK1V3Q2dZSUtvWkl6ajBFQXdJRFNBQXdSUUlnUXNkeDI2aDQvZTQ4MUxkWGY5RVNrV0Fwa1kybmFKaUgKZmNjQmxxVU1JejBDSVFDOGR5TGVoeENGYitnZnZkWXRZUlVJL1FSbGYvOUxsT2VBZ0lxVGt2V29uZz09Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K
    server: https://10.128.0.25:6443
  name: default
contexts:
- context:
    cluster: default
    user: default
  name: default
current-context: default
kind: Config
preferences: {}
users:
- name: default
  user:
    client-certificate-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJrakNDQVRlZ0F3SUJBZ0lJY1dtWkhqdTYwSVV3Q2dZSUtvWkl6ajBFQXdJd0l6RWhNQjhHQTFVRUF3d1kKYXpOekxXTnNhV1Z1ZEMxallVQXhOamcwTVRVeU5EUXhNQjRYRFRJek1EVXhOVEV5TURjeU1Wb1hEVEkwTURVeApOREV5TURjeU1Wb3dNREVYTUJVR0ExVUVDaE1PYzNsemRHVnRPbTFoYzNSbGNuTXhGVEFUQmdOVkJBTVRESE41CmMzUmxiVHBoWkcxcGJqQlpNQk1HQnlxR1NNNDlBZ0VHQ0NxR1NNNDlBd0VIQTBJQUJOaWVqSHRRY3MxdGFGMEQKM2VLUFVpcHNkVTgxUHBlT2I0QnhtMGpRZjBEdjd5S1FmaXZxbUNUZ1U3clFwT2ljbkJLbnk5ZVFNUWl2TklhWgpqUHlsbklxalNEQkdNQTRHQTFVZER3RUIvd1FFQXdJRm9EQVRCZ05WSFNVRUREQUtCZ2dyQmdFRkJRY0RBakFmCkJnTlZIU01FR0RBV2dCVCtJRXNRaDI4eEpnMVovVTFHbnhkMTc1UENsekFLQmdncWhrak9QUVFEQWdOSkFEQkcKQWlFQWlQUXRYdW1uRDB1RkZJMU1VbTZBNzE5RzJFRDdGUVhjbVdXUFRpYUZ1dWtDSVFDbUswbklNSmw2WEJBZwpPZlpZQXBiSUl1TC9rcGtaeE56N1ArYXJRVkVzVWc9PQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCi0tLS0tQkVHSU4gQ0VSVElGSUNBVEUtLS0tLQpNSUlCZGpDQ0FSMmdBd0lCQWdJQkFEQUtCZ2dxaGtqT1BRUURBakFqTVNFd0h3WURWUVFEREJock0zTXRZMnhwClpXNTBMV05oUURFMk9EUXhOVEkwTkRFd0hoY05Nak13TlRFMU1USXdOekl4V2hjTk16TXdOVEV5TVRJd056SXgKV2pBak1TRXdId1lEVlFRRERCaHJNM010WTJ4cFpXNTBMV05oUURFMk9EUXhOVEkwTkRFd1dUQVRCZ2NxaGtqTwpQUUlCQmdncWhrak9QUU1CQndOQ0FBUmJkWjdQMlRpK0c2UmplSU15MXkvY2tab2ZVdENUQ24zdEZDN29aM05iCmpVQmpldWpzQ1MyMXZscy83TS9vY1ZSVzV2N1dVOXEvYUd5dXFhd2NINXlubzBJd1FEQU9CZ05WSFE4QkFmOEUKQkFNQ0FxUXdEd1lEVlIwVEFRSC9CQVV3QXdFQi96QWRCZ05WSFE0RUZnUVUvaUJMRUlkdk1TWU5XZjFOUnA4WApkZStUd3Bjd0NnWUlLb1pJemowRUF3SURSd0F3UkFJZ0tpdVdhTlpPdnlON0RoamRsYml1MXFmTHBJamFEaEphCmFSeE9YeGhxYUo4Q0lFcGE4VUhHSU93aDZpcjJrN0N0VWN2dEZaS2JQV2FiNDZ5NHdoK2pRVzFHCi0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K
    client-key-data: LS0tLS1CRUdJTiBFQyBQUklWQVRFIEtFWS0tLS0tCk1IY0NBUUVFSUhGUFdCY3FLKzFoYlVXUEc4cGlrVGFkd09DbVVBYi8yd2NJT0ZpZVpCbFBvQW9HQ0NxR1NNNDkKQXdFSG9VUURRZ0FFMko2TWUxQnl6VzFvWFFQZDRvOVNLbXgxVHpVK2w0NXZnSEdiU05CL1FPL3ZJcEIrSytxWQpKT0JUdXRDazZKeWNFcWZMMTVBeENLODBocG1NL0tXY2lnPT0KLS0tLS1FTkQgRUMgUFJJVkFURSBLRVktLS0tLQo=


$ kubectl version
WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.
Client Version: version.Info{Major:"1", Minor:"27", GitVersion:"v1.27.1", GitCommit:"4c9411232e10168d7b050c49a1b59f6df9d7ea4b", GitTreeState:"clean", BuildDate:"2023-04-14T13:21:19Z", GoVersion:"go1.20.3", Compiler:"gc", Platform:"linux/amd64"}
Kustomize Version: v5.0.1
Server Version: version.Info{Major:"1", Minor:"26", GitVersion:"v1.26.4+k3s1", GitCommit:"8d0255af07e95b841952563253d27b0d10bd72f0", GitTreeState:"clean", BuildDate:"2023-04-20T00:33:18Z", GoVersion:"go1.19.8", Compiler:"gc", Platform:"linux/amd64"}




# 만약 권한이 644 이라면 보안을 위해서 600 으로 변경해 준다.

$ ll  ~/.kube/config
-rw-rw-r-- 1 ktdseduuser ktdseduuser 2963 May 15 12:49 /home/ktdseduuser/.kube/config

# 자신만 RW 권한 부여
$ chmod 600 ~/.kube/config

$ ll ~/.kube/config
-rw------- 1 ktdseduuser ktdseduuser 2957 May 13 14:35 /home/ktdseduuser/.kube/config

```





### (3) apt install 

```sh
# root 로

$ apt update

$ apt install vim


$ apt install tree


$ apt install iputils-ping

$ apt install net-tools


$ apt install netcat


$ apt install unzip


$ apt install git
 


$ apt install podman
$ podman version
Version:      3.4.2
API Version:  3.4.2
Go Version:   go1.15.2
Built:        Thu Jan  1 09:00:00 1970
OS/Arch:      linux/amd64


```









### (4) alias 정의

```sh
# user 권한으로

$ cat > ~/env
alias k='kubectl'
alias kk='kubectl -n kube-system'
alias ky='kubectl -n yjsong'
alias ki='kubectl -n istio-system'
alias kb='kubectl -n bookinfo'
alias kii='kubectl -n istio-ingress'
alias kkf='kubectl -n kafka'
alias krs='kubectl -n redis-system'

## alias 를 적용하려면 source 명령 수행
$ source ~/env

# booting 시 자동인식하게 하려면 아래 파일 수정
$ vi ~/.bashrc
...
source ~/env


```









### (5) helm install

#### helm client download

```sh
# 개인 PC WSL
# root 권한으로 수행
$ su


## 임시 디렉토리를 하나 만들자.
$ mkdir -p ~/temp/helm/
  cd ~/temp/helm/

# 다운로드
$ wget https://get.helm.sh/helm-v3.12.0-linux-amd64.tar.gz

# 압축해지
$ tar -zxvf helm-v3.12.0-linux-amd64.tar.gz

# 확인
$ ll linux-amd64/helm
-rwxr-xr-x 1 1001 docker 50597888 May 11 01:35 linux-amd64/helm*

# move
$ mv linux-amd64/helm /usr/local/bin/helm

# 확인
$ ll /usr/local/bin/helm*
-rwxr-xr-x 1 1001 docker 50597888 May 11 01:35 /usr/local/bin/helm*


# 일반유저로 복귀
$ exit


# 확인
$ helm version
version.BuildInfo{Version:"v3.12.0", GitCommit:"c9f554d75773799f72ceef38c51210f1842a1dea", GitTreeState:"clean", GoVersion:"go1.20.3"}



# 혹시 아래 메시지가 뜨면 권한조정한다.
# WARNING: Kubernetes configuration file is group-readable. This is insecure.
$ chmod 600 ~/.kube/config-multi


$ helm -n yjsong ls
NAME    NAMESPACE       REVISION        UPDATED STATUS  CHART   APP VERSION

```



#### [참고] bitnami repo 추가

- 유명한 charts 들이모여있는 bitnami repo 를 추가해 보자.

```sh
# test# add stable repo
$ helm repo add bitnami https://charts.bitnami.com/bitnami

$ helm repo list

$ helm search repo bitnami
# bitnami 가 만든 다양한 오픈소스 샘플을 볼 수 있다.
NAME                                            CHART VERSION   APP VERSION     DESCRIPTION
bitnami/airflow                                 14.1.3          2.6.0           Apache Airflow is a tool to express and execute...
bitnami/apache                                  9.5.3           2.4.57          Apache HTTP Server is an open-source HTTP serve...
bitnami/appsmith                                0.3.2           1.9.19          Appsmith is an open source platform for buildin...
bitnami/argo-cd                                 4.7.2           2.6.7           Argo CD is a continuous delivery tool for Kuber...
bitnami/argo-workflows                          5.2.1           3.4.7           Argo Workflows is meant to orchestrate Kubernet...
bitnami/aspnet-core                             4.1.1           7.0.5           ASP.NET Core is an open-source framework for we...
bitnami/cassandra                               10.2.2          4.1.1           Apache Cassandra is an open source distributed ...
bitnami/consul                                  10.11.2         1.15.2          HashiCorp Consul is a tool for discovering and ...
...

$ kubectl create ns yjsong

# 설치테스트(샘플: nginx)
$ helm -n yjsong install nginx bitnami/nginx

$ kubectl -n yjsong get all
NAME                         READY   STATUS              RESTARTS   AGE
pod/nginx-68c669f78d-wgnp4   0/1     ContainerCreating   0          10s

NAME            TYPE           CLUSTER-IP    EXTERNAL-IP   PORT(S)        AGE
service/nginx   LoadBalancer   10.43.197.4   <pending>     80:32754/TCP   10s

NAME                    READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/nginx   0/1     1            0           10s

NAME                               DESIRED   CURRENT   READY   AGE
replicaset.apps/nginx-68c669f78d   1         1         0       10s

# 간단하게 nginx 에 관련된 deployment / service / pod 들이 설치되었다.


# 설치 삭제
$ helm -n yjsong delete nginx

$ kubectl -n yjsong  get all
No resources found in yjsong namespace.
```







## 5) K9S Setup

kubernetes Cluster를 관리하기 위한 kubernetes cli tool 을 설치해 보자.

```sh
# root 권한으로

$ mkdir ~/temp/k9s
  cd  ~/temp/k9s

$ wget https://github.com/derailed/k9s/releases/download/v0.27.4/k9s_Linux_amd64.tar.gz
$ tar -xzvf k9s_Linux_amd64.tar.gz

$ ll
-rw-r--r-- 1  501 staff    10174 Mar 22  2021 LICENSE
-rw-r--r-- 1  501 staff    35702 May  7 16:54 README.md
-rwxr-xr-x 1  501 staff 60559360 May  7 17:01 k9s*
-rw-r--r-- 1 root root  18660178 May  7 17:03 k9s_Linux_amd64.tar.gz

$ cp ./k9s /usr/local/bin/

$ ll /usr/local/bin/
-rwxr-xr-x  1 root root 60559360 May 15 13:05 k9s*


# 일반 사용자로 전환
$ exit 

# 실행
$ k9s

```





# 3. GCP Load Balance Setup

외부에서 http 통신으로 접근할 수 있도록 LB 를 생성하여 Global IP를 만들고 kubernetes router 에 매핑 작업을 할 것이다.



##  [참고1] ingress controller

```sh
$ kubectl -n kube-system get svc
NAME             TYPE           CLUSTER-IP      EXTERNAL-IP                                                               PORT(S)                      AGE
kube-dns         ClusterIP      10.43.0.10      <none>                                                                    53/UDP,53/TCP,9153/TCP       74m
metrics-server   ClusterIP      10.43.148.138   <none>                                                                    443/TCP                      74m
traefik          LoadBalancer   10.43.189.44    10.128.0.25,10.128.0.26,10.128.0.27,10.128.0.28,10.128.0.29,10.158.0.25   80:31975/TCP,443:32562/TCP   73m
---
NAME             TYPE           CLUSTER-IP     EXTERNAL-IP               PORT(S)                      AGE
kube-dns         ClusterIP      10.43.0.10     <none>                    53/UDP,53/TCP,9153/TCP       13m
metrics-server   ClusterIP      10.43.100.52   <none>                    443/TCP                      13m
traefik          LoadBalancer   10.43.73.178   10.128.0.35,10.128.0.36   80:31353/TCP,443:32192/TCP   13m


```

traefic 이라는 Proxy tool 이 node port로 접근하여 routing 한다는 사실을 알 수 있다.

* http : 80 : 31353
* https : 443 : 32192



##  [참고2] GCP Load balance

GCP 에 LB 를 생성하여 고정 IP 를 할당받고 traefic 을 매핑해 보자.

Load Balace(부하분산) 를 만들기 위해서는 아래 순서로 진행해야 한다.

```
인스턴스 그룹생성 > 부하분산 생성 > 상태확인 생성 > 백엔드서비스 생성
```



## 1) LB 생성 - k3s Ingress (http)



### (1) 인스턴스 그룹 생성

* master 는 LB 와 연결 목적으로 생성한다.
* Status: Unmanaged
  * "New unmanaged instance group" 선택
* 이름 : ktds-k3s-master-group
* 구성원 : ktds-k3s-master01, 02 , 03
* 포트매핑
  * http : 31353



### (2) 상태확인(Status Check)

상태확인 생성

* 이름 : ktds-tcp-master-group-status-check
* 범위
  * 전역   <-- 선택하지 않음
  * 리전: us-central1(아이오와)      <-- 인스턴스 그룹이 있는 리전으로 선택
* 프로토콜 : TCP
* port : 31353
* 프록시 프로토콜
  * 없음               <-- 없음으로 선택
  * PROXY_V1     <-- 선택하지 않음
* 기타 : 기본값



### (3) 부하분산기 생성 - ★

* master 는 LB 와 연결 목적으로 생성한다.
* Status: Unmanaged
  * "New unmanaged instance group" 선택
* 이름 : ktds-tcp-master-group
* 구성원 : ktds-k3s-master01, 02 , 03
* 포트매핑
  * kafkaport  : 32100
  * kafkaport0 : 32200
  * kafkaport1 : 32201
  * kafkaport2 : 32202

* 메뉴 : 네트워크 서비스 > 부하분산
* protocol 선택
  * http   <-- 선
  * tcp
  * udp
* 인터넷 연결 또는 내부 전용
  * 인테넷에서 VM 연결
* 전역 또는 리전
  * 전역 HTTP 부하분산기
  * 전역 HTTP 부하분산기(기본)   <-- 이걸선택하자.
  * 리전별 HTTP 부하분산기(표준 네트워크 등급에 권장)  <-- 이걸선택하면 전용 subnet 이 필요하다. proxy 에러 발생한다.

* 이름 : ktds-k3s-lb
* 리전 ( 리전별 HTTP 부하분산기 선택시 )
  * 전역 <-- 선택하지 않음
  * us-central1(아이오와)      <-- 인스턴스 그룹이 있는 리전으로 선택
* frontend
  * 리전 : us-central1(아이오와)
  * 이름 : ktds-k3s-lb-frontend
  * 프로토콜 : http
  * 네트워크 서비스 계층 - 수준선택
    * 프리미엄
    * 표준      <-- 이걸로 선택
  * 포트 : 80
  * 새 고정 IP 주소
    * 주소이름 : ktds-k3s-lb-frontend-ip
    * IP
      * 35.209.207.26  : 80    (표준선택시)  <--         ★★★
      * 35.111.106.168  : 80    (프리미엄선택시)

* 백엔드 서비스 생성
  * 백엔드 만들기 선택
  * 상위구성
    * 이름 : ktds-k3s-master-backend-service-global
    * 리전: us-central1(아이오와)       <-- 인스턴스 그룹이 있는 리전으로 선택
    * 부하분산기유형 : 리전별 외부 http 부하분산
    * 벡엔유형 : 인스턴스 그룹

  * 벡엔드
    * 인스턴스 그룹 : ktds-k3s-master-group
    * port : 31353

  * Cloud CDN : 선택하지 않음
  * 상태확인 생성
    * 이름 : ktds-k3s-master-group-status-check-http
    * 범위
      * 전역   <-- 선택하지 않음
      * 리전: us-central1(아이오와)      <-- 인스턴스 그룹이 있는 리전으로 선택
    * 프로토콜 : TCP
    * port : 31353
    * 프록시 프로토콜
      * 없음               <-- 없음으로 선택
      * PROXY_V1     <-- 선택하지 않음
    * 기타 : 기본값

* 라우팅 규칙
  * 단순한 호스트 및 경로규칙
* 완료





### (4) 방화벽 규칙 만들기

반드시 대상 port 로 방화벽을 열어줘야 한다.

* 메뉴 : VPC네트워크 > 방화벽 > VPC 방화벽 규칙
* 방화벽 규칙 만들기
  * 규칙명 : allow-k3s-ingress-31353
  * 로그 : 사용안함
  * 우선순위 : 1000
  * 트래픽방향 : 수신
  * 일치시작업 : 허용
  * 대상 : 네트워크의 모든 인스턴스    <-- 매우 중요함...  잘못 선택해서 고생함.
  * 소스필터 : IPv4범위
  * 소스 IPv4범위
    * 35.209.207.26/32    <-- 로드 발란서를 통해서만 가능하도록 <-- 사용하지 말자. 그냥 아래 처럼 모두 열자.
      * 이렇게 지정하면 어디선가에서 잘 안됨.
    * 0.0.0.0/0     <-- 모두 열자.
  * 대상필터 : IPv4범위
  * 대상 IPv4범위 : 0.0.0.0/0    <-- 모든 소스 를 다 허용
  * port
    * tcp : 31353



### (5) nc 테스트

외부( local PC ) 에서 테스트 수행

```sh
$ nc -zv 35.209.207.26 80
```





## 2) LB 생성 - k3s Ingress (https)



### (1) 인스턴스 그룹 생성

* Status: Unmanaged
  * "New unmanaged instance group" 선택
* 이름 : ktds-k3s-master-group-https
* 구성원 : ktds-k3s-master01, 02 , 03
* 포트매핑
  * https : 32562



### (2) 부하분산기 생성

https 부하분산기 생성은 인증서 가 필수이다.



* protocol 선택
  * http   <-- 선
  * tcp
  * udp
* 인터넷 연결 또는 내부 전용
  * 인테넷에서 VM 연결
* 전역 또는 리전
  * 전역 HTTP 부하분산기
  * 전역 HTTP 부하분산기(기본)   <-- 이걸선택하자.
  * 리전별 HTTP 부하분산기(표준 네트워크 등급에 권장)  <-- 이걸선택하면 전용 subnet 이 필요하다. proxy 에러 발생한다.

* 이름 : ktds-k3s-lb-https
* 리전 ( 리전별 HTTP 부하분산기 선택시 )
  * 전역 <-- 선택하지 않음
  * us-central1(아이오와)      <-- 인스턴스 그룹이 있는 리전으로 선택
* frontend
  * 리전 : us-central1(아이오와)
  * 이름 : ktds-k3s-lb-frontend-https
  * 프로토콜 : https
  * 네트워크 서비스 계층 - 수준선택
    * 프리미엄
    * 표준      <-- 이걸로 선택
  * 포트 : 443
  * 인증서 : 필수  <--★★★
  * 새 고정 IP 주소
    * 주소이름 : ktds-k3s-lb-frontend-ip
    * IP
      * 35.209.207.26  : 80    (표준선택시)  <--         ★★★
      * 35.111.106.168  : 80    (프리미엄선택시)

* 백엔드 서비스 생성
  * 백엔드 만들기 선택
  * 상위구성
    * 이름 : ktds-k3s-master-backend-service-global
    * 리전: us-central1(아이오와)       <-- 인스턴스 그룹이 있는 리전으로 선택
    * 부하분산기유형 : 리전별 외부 http 부하분산
    * 벡엔유형 : 인스턴스 그룹

  * 벡엔드
    * 인스턴스 그룹 : ktds-k3s-master-group
    * port : 31353

  * Cloud CDN : 선택하지 않음
  * 상태확인 생성
    * 이름 : ktds-k3s-master-group-status-check-https
    * 범위
      * 전역   <-- 선택하지 않음
      * 리전: us-central1(아이오와)      <-- 인스턴스 그룹이 있는 리전으로 선택
    * 프로토콜 : TCP
    * port : 32562
    * 프록시 프로토콜
      * 없음               <-- 없음으로 선택
      * PROXY_V1     <-- 선택하지 않음
    * 기타 : 기본값

* 라우팅 규칙
  * 단순한 호스트 및 경로규칙
* 완료



## 3) http sample app deploy

userlist app 을 deploy 하여 ingress 까지확인해보자.



### (1) 개인별 Namespace 생성

```sh
$ kubectl create ns yjsong

$ alias ku='kubectl -n yjsong'

```






### (2) 실습자료 download

```sh
# 일반계정에서 수행

## githubrepo directory 생성
$ mkdir -p ~/yjsong/githubrepo

$ cd ~/yjsong/githubrepo

$ git clone https://github.com/ssongman/ktds-edu-k8s-istio.git
Cloning into 'ktds-edu-k8s-istio'...
remote: Enumerating objects: 69, done.
remote: Counting objects: 100% (69/69), done.
remote: Compressing objects: 100% (55/55), done.
remote: Total 69 (delta 15), reused 62 (delta 11), pack-reused 0
Unpacking objects: 100% (69/69), 1.63 MiB | 4.09 MiB/s, done.

$ ll ~/yjsong/githubrepo
drwxrwxr-x 7 ktdseduuser ktdseduuser 4096 May 13 17:36 ktds-edu-k8s-istio/

$ cd ~/yjsong/githubrepo/ktds-edu-k8s-istio/
```





### (2) Deployment/Service

- yaml 생성

```sh
$ cd ~/yjsong/githubrepo/ktds-edu-k8s-istio


# ku 로 alias 선언
$ alias ku='kubectl -n yjsong'

# deployment yaml 확인
$ cat ./kubernetes/userlist/11.userlist-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: userlist
  labels:
    app: userlist
spec:
  replicas: 1
  selector:
    matchLabels:
      app: userlist
  template:
    metadata:
      labels:
        app: userlist
    spec:
      containers:
      - name: userlist
        image: ssongman/userlist:v1
        ports:
        - containerPort: 8181

# deployment 생성
$ ku create -f ./kubernetes/userlist/11.userlist-deployment.yaml

# deployment 확인
$ ku get deployment
NAME       READY   UP-TO-DATE   AVAILABLE   AGE
userlist   0/1     1            0           12s

# pod 확인
$ ku get pod
NAME                       READY   STATUS              RESTARTS   AGE
userlist-bfd857685-g6kj6   0/1     ContainerCreating   0          19s


# service yaml 확인
$ cat ./kubernetes/userlist/12.userlist-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: userlist-svc
spec:
  selector:
    app: userlist
  ports:
  - name: http
    protocol: TCP
    port: 80
    targetPort: 8181
  type: ClusterIP

# service 생성
$ ku create -f ./kubernetes/userlist/12.userlist-svc.yaml


# service 확인
$ ku get svc
NAME           TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
userlist-svc   ClusterIP   10.43.34.5   <none>        80/TCP    24s

```

- curltest 확인
  - curltest pod 내에 접근해서 테스트 시도

```sh
# curltest pod 생성
$ ku run curltest --image=curlimages/curl -- sleep 365d

$ ku get pod
NAME                       READY   STATUS    RESTARTS   AGE
curltest                   1/1     Running   0          12s
userlist-bfd857685-g6kj6   1/1     Running   0          115s


# curl test 수행
$ ku exec -it curltest -- curl userlist-svc/users/1
{"id":1,"name":"Eliezer Lind","gender":"F","image":"/assets/image/cat1.jpg"}

```

userlist-svc 라는 서비스명으로 접근이 잘 되는 것을 확인 할 수 있다.



### (4) Ingress 

- ingress controller 확인

```sh
$ kubectl -n kube-system get svc
NAME             TYPE           CLUSTER-IP     EXTERNAL-IP               PORT(S)                      AGE
kube-dns         ClusterIP      10.43.0.10     <none>                    53/UDP,53/TCP,9153/TCP       40m
metrics-server   ClusterIP      10.43.100.52   <none>                    443/TCP                      40m
traefik          LoadBalancer   10.43.73.178   10.128.0.35,10.128.0.36   80:31353/TCP,443:32192/TCP   40m

```

traefic 이라는 Proxy tool 이 node port (31353) 로 접근하여 routing 한다는 사실을 알 수 있다.

이미 GCP Load balance  를 생성하여 공인IP 가 할당되어 있으며 해당 IP 가 L4 역할을 수행한다.

해당 공인 IP 와 위 traefik controller 의 node port가 서로 매핑되도록 설정작업을 해 놓았다.



- master node와 port-forwarding 정보

```
35.209.207.26 : 80   = master01/master02/master03 : 31353
35.209.207.26 : 443  = master01/master02/master03 : 32192
```

그러므로 우리는 35.209.207.26 : 80 으로 call 을 보내면 된다.  

대신 Cluster 내 진입후 자신의 service 를 찾기 위한 host address 를 같이 보내야 한다. (ingress 설정)



- 개인별 테스트를 위한 도메인 변경

아래 16.userlist-ingress-cloud.yaml 파일을 오픈하여 yjsong 부분을 본인의 계정명으로 변경하자.

```sh
$ cd ~/yjsong/githubrepo/ktds-edu-k8s-istio/

# ingress 확인
$ cat ./kubernetes/userlist/16.userlist-ingress-cloud.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: userlist-ingress
  annotations:
    kubernetes.io/ingress.class: "traefik"
spec:
  rules:
  - host: "userlist.yjsong.cloud.35.209.207.26.nip.io"       <-- yjsong 을 자신의 Namespace 명으로 수정
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: userlist-svc
            port:
              number: 80


# ingress 수정
$ vi ./kubernetes/userlist/16.userlist-ingress-cloud.yaml
...


# ingress 생성
$ ku create -f ./kubernetes/userlist/16.userlist-ingress-cloud.yaml

$ ku get ingress
NAME               CLASS    HOSTS                                        ADDRESS                                                                   PORTS   AGE
userlist-ingress   <none>   userlist.yjsong.cloud.35.209.207.26.nip.io   10.128.0.25,10.128.0.26,10.128.0.27,10.128.0.28,10.128.0.29,10.158.0.25   80      55m

# 서비스 확인
$ curl http://userlist.yjsong.cloud.35.209.207.26.nip.io/users/1
```





- 서버 terminal 에서 확인

```sh
# traefik node port 로 접근시도
# node 중 하나를 골라서 시도하자.  (master01_IP : 10.128.0.35)
$ curl http://10.128.0.35:31353/users/1 -H "Host:userlist.yjsong.cloud.35.209.207.26.nip.io"
{"id":1,"name":"Hester Yost","gender":"F","image":"/assets/image/cat1.jpg"}

# master01의 공인IP 로
$ curl http://35.224.7.108:31353/users/1 -H "Host:userlist.yjsong.cloud.35.209.207.26.nip.io"
{"id":1,"name":"Hester Yost","gender":"F","image":"/assets/image/cat1.jpg"}

# Load Balancer 로 접근
$ curl http://35.209.207.26:80/users/1 -H "Host:userlist.yjsong.cloud.35.209.207.26.nip.io"
{"id":1,"name":"Fay Abbott MD","gender":"F","image":"/assets/image/cat1.jpg"}

# Load Balancer 로 접근 ( 위와 동일한 방법임 )
$ curl http://userlist.yjsong.cloud.35.209.207.26.nip.io/users/1
{"id":1,"name":"Fay Abbott MD","gender":"F","image":"/assets/image/cat1.jpg"}
```

위 두개의 curl  을 잘 이해하자.

첫번째는 nodeport 를 통해서 접속을 시도한 경우이다.

두번째는 Cloud 에서 제공하는 공인 IP (Load Balancer)의 80 port 로 접속이 되었다.

즉, 위 도메인은 어디서든지 접속 가능한 상태이다.  확인을 위해서 로컬 크롬브라우저에서 접속을 시도해 보자.



### (5) trouble shooting

#### 50x error 발생

ingress 호출시 502 Server Error 나는 경우

```
502 Server Error 나는 경우

load balcer 가 정확히 연결되지 않았다.
아래의 원인을 찾아보자.
1) 상태점검(Status Check) 확인
2) 방화벽 점검

```



#### nslookup 으로 domain 확인

```sh
$ ping userlist.yjsong.cloud.35.209.207.26.nip.io
PING userlist.yjsong.cloud.35.209.207.26.nip.io (35.209.207.26) 56(84) bytes of data.



$ nslookup userlist.yjsong.cloud.35.209.207.26.nip.io
Server:         127.0.0.53
Address:        127.0.0.53#53

Non-authoritative answer:
Name:   userlist.yjsong.cloud.35.209.207.26.nip.io
Address: 35.209.207.26


$ nslookup -type=ns  35.209.207.26
Server:         127.0.0.53
Address:        127.0.0.53#53

Non-authoritative answer:
26.207.209.35.in-addr.arpa      name = 26.207.209.35.bc.googleusercontent.com.



```







#### tracert 확인

경로추적

```sh
$ traceroute userlist.yjsong.cloud.35.209.207.26.nip.io

traceroute to userlist.yjsong.cloud.35.209.207.26.nip.io (35.209.207.26), 30 hops max, 60 byte packets
 1  * * *
 2  * * *
 3  * * *
 4  * * *
 5  * * *
 6  * * *
 7  * * *
 8  * * *
 9  * * *
10  * * *
11  26.207.209.35.bc.googleusercontent.com (35.209.207.26)  0.702 ms * *


$ traceroute userlist.yjsong.cloud.35.209.207.26.nip.io -p 80

traceroute to userlist.yjsong.cloud.35.209.207.26.nip.io (35.209.207.26), 30 hops max, 60 byte packets
 1  * * *
 2  * * *
 3  * * *
 4  * * *
 5  * * *
 6  * * *
 7  * * *
 8  * * *
 9  * * *
10  * * *
11  26.207.209.35.bc.googleusercontent.com (35.209.207.26)  0.677 ms  0.658 ms *


```







### (6) Clean Up

```sh
## userlist app 삭제
$ cd ~/yjsong/githubrepo/ktds-edu-k8s-istio

$ ku delete -f ./kubernetes/userlist/11.userlist-deployment.yaml
  ku delete -f ./kubernetes/userlist/12.userlist-svc.yaml
  ku delete -f ./kubernetes/userlist/16.userlist-ingress-cloud.yaml

$ ku delete pod curltest

$ ku get all

## githubrepo directory 삭제
$ cd ~/yjsong/githubrepo
$ rm -rf ktds-edu-k8s-istio

```







##  4) LB 생성 - kafka Nodeport

GCP 에 Kafka 전용 LB 를 생성하여 고정 IP 를 할당받고 traefic 을 매핑해 보자.



### (1) 상태확인(Status Check)

상태확인 생성

* 이름 : ktds-sc-kafka-broker0, 1, 2
* 범위
  * 전역   <-- 선택하지 않음
  * 리전: us-central1(아이오와)      <-- 인스턴스 그룹이 있는 리전으로 선택
* 프로토콜 : TCP
* port 
  * bootstrab  : 32100
  * broker0 : 32200
  * broker1 : 32201
  * broker2 : 32202
* 프록시 프로토콜
  * 없음               <-- 없음으로 선택
  * PROXY_V1     <-- 선택하지 않음
* 기타 : 기본값



### (2) 부하분산기 생성

* 메뉴 : 네트워크 서비스 > 부하분산
* protocol 선택
  * TCP 부하 분산 선택
  * 기본선택
  * 인터넷 연결 또는 내부 전용
    * 인테넷 트래픽을 VM으로 분산
  * 리전선택
    * 단일리전 선택
  * 부하분산기 유형
    * 패스스루  <-- 선택
    * 프록
  * 백엔드유형
    * 백엔드 서비스
    * 대상 풀 및 대상 인스턴스 <--- 선택
* 부하분산기 이름
  * 이름: ktds-lb-kafka-bootstrab,  ktds-lb-kafka-broker0, 1, 2
  * 리전 : 아이오와 us-central1  <-- backend Group 서버들 위치와동일해야함.
* 백엔드 구성
  * 벡엔드
    * 기존인스턴스 선택 <-- 선택
      * master1,2,3 선택
* frontend
  * 리전 : us-central1(아이오와)
  * 이름 : ktds-frontend-kafka-bootstrab, ktds-frontend-kafka-broker0, 1, 2
  * 프로토콜 : tcp
  * 네트워크 서비스 계층
    * 프리미엄
    * 표준      <-- 이걸로 선택
  * IP 주소
    * 주소이름 : ktds-k3s-lb-frontend-ip
    * IP : 35.209.207.26
  * 포트
    * 단일  <-- 이걸 선택
      * bootstrab  : 32100
      * broker0 : 32200
      * broker1 : 32201
      * broker2 : 32202
    * 다중 
    * 전체



### (3) 방화벽 규칙 만들기

반드시 대상 port 로 방화벽을 열어줘야 한다.

* 메뉴 : VPC네트워크 > 방화벽 > VPC 방화벽 규칙
* 방화벽 규칙 만들기
  * 규칙명 : allow-kafka-nodeport
  * 로그 : 사용안함
  * 우선순위 : 1000
  * 트래픽방향 : 수신
  * 일치시작업 : 허용
  * 대상 : 네트워크의 모든 인스턴스    <-- 매우 중요함...  잘못 선택해서 고생함.
  * 소스필터 : IPv4범위
  * 소스 IPv4범위
    * 35.209.207.26/32    <-- 로드 발란서를 통해서만 가능하도록 <-- 사용하지 말자. 그냥 아래 처럼 모두 열자.
      * 이렇게 지정하면 어디선가에서 잘 안됨.
    * 0.0.0.0/0     <-- 모두 열자.
  * 대상필터 : IPv4범위
  * 대상 IPv4범위 : 0.0.0.0/0    <-- 모든 소스 를 다 허용
  * port
    * tcp
      * bootstrab  : 32100
      * broker0 : 32200
      * broker1 : 32201
      * broker2 : 32202



### (4) nc 테스트

외부( local PC ) 에서 테스트 수행

```sh
$ nc -zv 35.209.207.26 32100
$ nc -zv 35.209.207.26 32200
$ nc -zv 35.209.207.26 32201
$ nc -zv 35.209.207.26 32202

```











##  5) LB 생성 - redis Nodeport

GCP 에 redis 전용 LB 를 생성하여 고정 IP 를 할당받고 traefic 을 매핑해 보자.



### (1) 상태확인(Status Check)

상태확인 생성

* 이름 : ktds-sc-redis-master, ktds-sc-redis-replicas
* 범위
  * 전역   <-- 선택하지 않음
  * 리전: us-central1(아이오와)      <-- 인스턴스 그룹이 있는 리전으로 선택
* 프로토콜 : TCP
* port 
  * master: 32300
  * replicas: 32310
* 프록시 프로토콜
  * 없음               <-- 없음으로 선택
  * PROXY_V1     <-- 선택하지 않음
* 기타 : 기본값



### (2) 부하분산기 생성

* 메뉴 : 네트워크 서비스 > 부하분산
* protocol 선택
  * TCP 부하 분산 선택
  * 기본선택
  * 인터넷 연결 또는 내부 전용
    * 인테넷 트래픽을 VM으로 분산
  * 리전선택
    * 단일리전 선택
  * 부하분산기 유형
    * 패스스루  <-- 선택
    * 프록
  * 백엔드유형
    * 백엔드 서비스
    * 대상 풀 및 대상 인스턴스 <--- 선택
* 부하분산기 이름
  * 이름: ktds-lb-redis-master, ktds-lb-redis-replicas
  * 리전 : 아이오와 us-central1  <-- backend Group 서버들 위치와동일해야함.
* 백엔드 구성
  * 벡엔드
    * 기존인스턴스 선택 <-- 선택
      * master1,2,3 선택
* frontend
  * 리전 : us-central1(아이오와)
  * 이름 : ktds-frontend-redis-master, ktds-frontend-redis-replicas
  * 프로토콜 : tcp
  * 네트워크 서비스 계층
    * 프리미엄
    * 표준      <-- 이걸로 선택
  * IP 주소
    * 주소이름 : ktds-k3s-lb-frontend-ip
    * IP : 35.209.207.26
  * 포트
    * 단일  <-- 이걸 선택
      * master: 32300
      * replicas: 32310
    * 다중 
    * 전체



### (3) 방화벽 규칙 만들기

반드시 대상 port 로 방화벽을 열어줘야 한다.

* 메뉴 : VPC네트워크 > 방화벽 > VPC 방화벽 규칙
* 방화벽 규칙 만들기
  * 규칙명 : allow-redis-nodeport
  * 로그 : 사용안함
  * 우선순위 : 1000
  * 트래픽방향 : 수신
  * 일치시작업 : 허용
  * 대상 : 네트워크의 모든 인스턴스    <-- 매우 중요함...  잘못 선택해서 고생함.
  * 소스필터 : IPv4범위
  * 소스 IPv4범위
    * 35.209.207.26/32    <-- 로드 발란서를 통해서만 가능하도록 <-- 사용하지 말자. 그냥 아래 처럼 모두 열자.
      * 이렇게 지정하면 어디선가에서 잘 안됨.
    * 0.0.0.0/0     <-- 모두 열자.
  * 대상필터 : IPv4범위
  * 대상 IPv4범위 : 0.0.0.0/0    <-- 모든 소스 를 다 허용
  * port
    * tcp
      * master: 32300
      * replicas: 32310



### (4) nc 테스트

외부( local PC ) 에서 테스트 수행

```sh

# master node
$ nc -zv 35.209.207.26 32300
Connection to 35.209.207.26 32300 port [tcp/*] succeeded!


# slave node
$ nc -zv 35.209.207.26 32310
Connection to 35.209.207.26 32310 port [tcp/*] succeeded!
```







# 4. helm install

## 1) helm client download

helm client 를 local 에 설치해 보자.

```sh
# root 권한으로 수행
$ su

## 임시 디렉토리를 하나 만들자.
$ mkdir -p ~/temp/helm/
  cd ~/temp/helm/

# 다운로드
$ wget https://get.helm.sh/helm-v3.12.0-linux-amd64.tar.gz

# 압축해지
$ tar -zxvf helm-v3.12.0-linux-amd64.tar.gz

# 확인
$ ll linux-amd64/helm
-rwxr-xr-x 1 ktdseduuser 123 50597888 May 10 16:35 linux-amd64/helm*


# move
$ mv linux-amd64/helm /usr/local/bin/helm

# 확인
$ ll /usr/local/bin/helm*
-rwxr-xr-x 1 1001 docker 50597888 May 11 01:35 /usr/local/bin/helm*


# 일반유저로 복귀
$ exit


# 확인
$ helm version
version.BuildInfo{Version:"v3.12.0", GitCommit:"c9f554d75773799f72ceef38c51210f1842a1dea", GitTreeState:"clean", GoVersion:"go1.20.3"}


$ helm -n yjsong ls
NAME    NAMESPACE       REVISION        UPDATED STATUS  CHART   APP VERSION

```



## [참고] bitnami repo 추가

- 유명한 charts 들이모여있는 bitnami repo 를 추가해 보자.

```sh
# test# add stable repo
$ helm repo add bitnami https://charts.bitnami.com/bitnami

$ helm search repo bitnami
# bitnami 가 만든 다양한 오픈소스 샘플을 볼 수 있다.
NAME                                            CHART VERSION   APP VERSION     DESCRIPTION
bitnami/airflow                                 14.1.3          2.6.0           Apache Airflow is a tool to express and execute...
bitnami/apache                                  9.5.3           2.4.57          Apache HTTP Server is an open-source HTTP serve...
bitnami/appsmith                                0.3.2           1.9.19          Appsmith is an open source platform for buildin...
bitnami/argo-cd                                 4.7.2           2.6.7           Argo CD is a continuous delivery tool for Kuber...
bitnami/argo-workflows                          5.2.1           3.4.7           Argo Workflows is meant to orchestrate Kubernet...
bitnami/aspnet-core                             4.1.1           7.0.5           ASP.NET Core is an open-source framework for we...
bitnami/cassandra                               10.2.2          4.1.1           Apache Cassandra is an open source distributed ...
...

# 설치테스트(샘플: nginx)
$ helm -n song install nginx bitnami/nginx

$ ks get all
NAME                         READY   STATUS              RESTARTS   AGE
pod/nginx-68c669f78d-wgnp4   0/1     ContainerCreating   0          10s

NAME            TYPE           CLUSTER-IP    EXTERNAL-IP   PORT(S)        AGE
service/nginx   LoadBalancer   10.43.197.4   <pending>     80:32754/TCP   10s

NAME                    READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/nginx   0/1     1            0           10s

NAME                               DESIRED   CURRENT   READY   AGE
replicaset.apps/nginx-68c669f78d   1         1         0       10s

# 간단하게 nginx 에 관련된 deployment / service / pod 들이 설치되었다.


# 설치 삭제
$ helm -n song delete nginx

$ ks get all
No resources found in yjsong namespace.
```





# 5. 기타 VM 설정





## 1) Docker 설치

 

### 1. 우분투 시스템 패키지 업데이트

```
sudo apt-get update
```

### 2. 필요한 패키지 설치

```
sudo apt-get install apt-transport-https ca-certificates curl gnupg-agent software-properties-common
```

 

### 3. Docker의 공식 GPG키를 추가

```
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
```

 

### 4. Docker의 공식 apt 저장소를 추가

```
sudo add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable"
```

 

### 5. 시스템 패키지 업데이트

```
sudo apt-get update
```

### 6. Docker 설치

```
sudo apt-get install docker-ce docker-ce-cli containerd.io
```

 

### 7. Docker가 설치 확인

#### 7-1 도커 실행상태 확인

```
sudo systemctl status docker
```

#### 7-2 도커 실행

```
sudo docker run hello-world
```



### 8. Docker 관련 계정 권한 부여

설치후 기본적으로 root 권한만 사용할 수 있다.

```sh
# version 확인
$ docker version
Client: Docker Engine - Community
 Version:           24.0.5
 API version:       1.43
 Go version:        go1.20.6
 Git commit:        ced0996
 Built:             Fri Jul 21 20:35:18 2023
 OS/Arch:           linux/amd64
 Context:           default
permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get "http://%2Fvar%2Frun%2Fdocker.sock/v1.24/version": dial unix /var/run/docker.sock: connect: permission denied



```



권한 변경해 보자.

```sh

# docker 관련 계정 권한 부여
$ sudo usermod -aG docker ktdseduuser

# service 재시작
$ sudo service docker restart

# 상태확인
$ sudo service docker status


```



재접속 필요

```sh

# version 확인
$ docker version

Client: Docker Engine - Community
 Version:           24.0.5
 API version:       1.43
 Go version:        go1.20.6
 Git commit:        ced0996
 Built:             Fri Jul 21 20:35:18 2023
 OS/Arch:           linux/amd64
 Context:           default

Server: Docker Engine - Community
 Engine:
  Version:          24.0.5
  API version:      1.43 (minimum version 1.12)
  Go version:       go1.20.6
  Git commit:       a61e2b4
  Built:            Fri Jul 21 20:35:18 2023
  OS/Arch:          linux/amd64
  Experimental:     false
 containerd:
  Version:          1.6.22
  GitCommit:        8165feabfdfe38c65b599c4993d227328c231fca
 runc:
  Version:          1.1.8
  GitCommit:        v1.1.8-0-g82f18fe
 docker-init:
  Version:          0.19.0
  GitCommit:        de40ad0

```











